\chapter{Optimization}

\section{Optimization in data analysis}
Lots of problem in statistical learning can be formulated as optimization problems. Thus, optimization is important because we need to know how solve the optimization problems in a computationally efficient way.

Assume that we have \textbf{random data} $D \in \mathcal{D}$ as feature-label pairs: $D = (X,Y)$.

\begin{definition}
    The \textbf{loss} function $Q_\beta(d)$ is a function of $d \in D$ parameterized by $\beta \in \mathbb{R}^p$.
\end{definition}
\begin{definition}
    The \textbf{cost} function, also known as \textit{risk} or \textit{objective} function, is defined as the loss function averaged over $D$:
    \[
        J(\beta) = \E{}{Q_\beta(D)}
    \]
\end{definition}

%Another thing is the more we relax the assumptions, the less information we get. \textit{The less we ask, the less we get.} For example, when I work under the assumption that

%A \textbf{performance guarantee} means that whatever we do the dataset seems to work, meaning that we can apparently obtain some information.

\begin{definition}
    We can define statistical learning as an optimization problem as the \textit{unconstrained minimization} of the \textit{cost function}:
    \[
        \min_{\beta \in \mathbb{R}^p} J(\beta) = \E{}{Q_\beta(D)}
    \]
\end{definition}

For example in the regression case we minimize the mean-square-error while in the classification case we minimize the conditional cross-entropy.

\paragraph*{Regression}
In the regression case, the loss function is the error:
\[
    Q_\beta(d) = Q_\beta(x,y) = (x^T\beta - y)^2
\]
Then the cost function will be the mean square error:
\[
    J(\beta) = \E{}{(X^T \beta - Y)^2}
\]

\paragraph*{Classification}
In the logistic regression case, the loss function is the cross-entropy:
\[
    Q_\beta(d) = Q_\beta(x,y) = \log\left(1+e^{-y x^T \beta}\right)
\]
Then the cost function will be the \textit{conditional} cross-entropy:
\[
    J(\beta) = \E{}{\log\left(1+e^{-y x^T \beta}\right)}
\]
\section{Problem assumptions}
\begin{remark}
    Given the cost function $J: \beta \in \mathbb{R}^{p} \to  j \in \mathbb{R}$, $\nabla J(\beta)$ is the gradient of the cost function and it is a vector of dimensions $p \times 1$, where each element is $\frac{\partial J(\beta)}{\partial \beta_i}$. The norm we are going to use is the \textbf{euclidean norm}.
\end{remark}

\subsection{Lipschitz Continuity}
First, remember that a function is continuous if two points become closer, then the values of the function evaluated in those two points also becomes closer, that is the difference between the two points becomes closer to zero.
\[
    x_1 - x_2 \to 0 \implies f(x_1) - f(x_2) \to 0
\]
When we say a function is continuous, we don't know that is the rate with which they become closer. This is why we need to introduce the \textbf{Lipschitz-continuity of the gradient}.

\begin{definition}
    A function is defined \textbf{Lipschitz continuous} if:
    \[
        \forall \beta_1, \beta_2 \colon \| \nabla J(\beta_2) - \nabla J(\beta_1)\| \leq \delta \|\beta_2 - \beta_1\|
    \]
    i.e. the difference of the gradient compute for the parameters $\beta_1, \beta_2$ is upper bounded by the difference of the parameters times a constant $\delta > 0$, called \textbf{Lipschitz constant}.
\end{definition}

When the gradient is Lipschitz-continuous we usually say \textit{the gradient varies smoothly}.

\begin{theorem}
    \[
        \text{Lipschitz continuity} \implies \text{continuity}
    \]
\end{theorem}
\subsection{Convexity}

\begin{definition}
    A \textbf{convex combination} is the sum of two quantites weighted respectively by the coefficient $p$ and $1-p$.
    \[
        c = p x_1 + (1-p) x_2
    \]
    By definition, the convex combination of $x_1$ and $x_2$ always lies in between of $x_1$ and $x_2$.
\end{definition}

\begin{definition}
    A function $f(x)$ is \textbf{convex} if for every $x_1, x_2$, $x_1 \neq x_2$ and for every $p \in (0,1)$, the function evaluated in the convex combination of $x_1$ and $x_2$ lies always below the chord that is described by the convex combination.
    \[
        f(p x_1 + (1-p) x_2) \leq p f(x_1) + (1-p) f(x_2) \qquad \forall \alpha \in (0,1) \text{ and } \forall x_1 \neq x_2
    \]
\end{definition}

Another assumption of the optimization problems in statistical learning is that the cost function $J(\beta)$ need to be \textit{convex}:
\[
    J(\alpha \beta_1 + (1-\alpha)\beta_2) \leq \alpha J(\beta_1) + (1-\alpha)J(\beta_2) \qquad  \forall \alpha \in (0,1) \text{ and } \forall \beta_1 \neq \beta_2
\]

% immagine 
The most frequent convention in optimization is doing \textbf{minimization}, and thus assuming that the cost function is convex. However, if we need to solve a \textit{maximization problem}, then we would need a \textit{concave} function.

\begin{definition}
    A function $J(\beta)$ is said to be \textbf{strictly convex} if:
    \[
        J(\alpha \beta_1 + (1-\alpha)\beta_2) < \alpha J(\beta_1) + (1-\alpha)J(\beta_2) \qquad \forall \alpha \in (0,1) \text{ and } \forall \beta_1 \neq \beta_2
    \]
\end{definition}

\begin{definition}
    A function $J(\beta)$ is said to  be \textbf{strongly convex} if:
    \[
        J(\alpha \beta_1 + (1-\alpha)\beta_2) \leq \alpha J(\beta_1) + (1-\alpha)J(\beta_2) - \frac{\nu}{2} \alpha (1-\alpha) \| \beta_1 - \beta_2 \|^2
    \]
    \[
        \forall \alpha \in (0,1) \text{ and } \forall \beta_1 \neq \beta_2
    \]
    where $\nu >0 $ is the \textbf{strong-convexity constant}.
\end{definition}

\begin{theorem}
    We have the following chain of implications:
    \[
        \text{strong convexity} \implies \text{strict convexity} \implies \text{convexity}
    \]
\end{theorem}
Since the term $\frac{\nu}{2} \alpha (1-\alpha) \| \beta_1 - \beta_2 \|^2$ is always positive, then we have that even if the function will be equal to the chord, it will be always under the chord, thus implying the strict convexity.
\paragraph*{Hessian matrix}
The Hessian matrix, indicated with $\nabla^2 J(\beta)$, is the matrix of second derivatives of a function. It is a matrix of dimensions $p \times p$, where each element is $\frac{\partial^2 J(\beta)}{\partial \beta_i \partial \beta_j}$.

\begin{definition}
    A matrix is said to be \textbf{positive definite} if all of its eigenvalues are positive. The notation $A > B$ for two symmetric matrices $A$ and $B$ means that $A-B$ is a \textbf{positive definite matrix}.
\end{definition}

\begin{definition}
    A matrix is said to be \textbf{positive semi-definite} if all of its eigenvalues are non-negative. The notation $A \geq B$ for two symmetric matrices $A$ and $B$ means that $A-B$ is a \textbf{positive semi-definite matrix}. We can also use the symbol $\succ$ and $\succeq$.
\end{definition}

We can rewrite the definitions of convexity in terms of the Hessian matrix.
\begin{definition}
    A function $J(\beta)$ is said to be \textbf{convex} if:
    \[
        \nabla^2 J(\beta) \geq 0 \ \forall \beta
    \]
\end{definition}

For example, let us consider the one dimensional case: if the second derivative is positive, then the function is convex.

\begin{definition}
    A function $J(\beta)$ is said to be \textbf{strictly convex} if:
    \[
        \nabla^2 J(\beta) > 0 \ \forall \beta
    \]
\end{definition}

\begin{definition}
    A function $J(\beta)$ is said to be \textbf{strongly convex} if:
    \[
        \nabla^2 J(\beta) \geq \nu I \ \forall \beta
    \]
    where $\nu > 0$ is the \textbf{strong-convexity constant}.
\end{definition}

The parabola is the most common example of a strongly convex function.

An example of function that is not strongly convex but convex is the exponential function $e^{-x}$, because we cannot find a $\nu$ such that the function is greater or equal than that threshold. This is also because the exponential function is bounded from below by zero.

\begin{theorem}
    The MSE cost used in linear regression is strongly convex if the covariance matrix $\E{}{X^TX}$ is invertible.
\end{theorem}

\begin{proof}
    First we are going to prove that the Hessian matrix of the MSE cost is $\nabla^2 J(\beta) = 2 \E{}{X^TX}$.

    Let $X \in \mathbb{R}^{1 \times p}$ be the matrix of the features, $\beta \in \mathbb{R}^{p \times 1}$ and $Y \in \mathbb{R}$ be the target variable.
    Let us compute the gradient of the MSE cost:
    \begin{align*}
        \nabla J(\beta) = \nabla_\beta \E{}{(X \beta - Y)^2} =  \E{}{\nabla_\beta(X \beta - Y)^2} \\ = \E{}{\nabla_\beta(\beta^T X^T X \beta - 2 \beta^T X^T Y + Y^T Y)} = \\
        = \E{}{2 X^T X \beta - 2 X^T Y}
    \end{align*}
    Then we can compute the Hessian matrix:
    \begin{align*}
        \nabla^2 J(\beta) = \nabla_\beta \nabla_\beta J(\beta) = \nabla_\beta \E{}{2 X^T X \beta - 2 X^T Y} = \\
        = \E{}{2 X^T X} = 2 \E{}{X^T X}
    \end{align*}

    Any covariance matrix is positive semi-definite and symmetric. Since the covariance matrix is positive semi-definite, then all of its eigenvalues are non-negative. Since the covariance matrix is then the determinant of the covariance matrix is non-zero, which means that all of its eigenvalues are non-zero. Thus, the eigenvalues of the covariance matrix are positive, and greater then the minimum eigenvalue $\lambda_{min} = \nu$, which proves the strong convexity of the MSE cost.
\end{proof}

\section{Gradient Descent}
Finding the minimum of a function is not an easy problem because we need to find the point where the derivative is zero; but this alone does not guarantee that we have found the minimum, because it could be a maximum or a saddle point. And even if we ascertain that the point is a point of minimum, we don't know if it is a local or global minimum.

So algorithms like the gradient descent are very useful when our problem has a complex formulation.
The basic idea of the gradient descent is to start from a random point and then move in the opposite direction of the gradient, which is the direction of the steepest descent. The gradient descent is an iterative algorithm.

\begin{algorithm}[H]
    \SetAlgoLined
    Set a starting point $\beta_0$ \\
    \For{$i=1,2,\dots$}{
        $\beta_{i} = \beta_{i-1} - \mu \nabla J\beta_{i-1}$
    }
    \caption{Gradient Descent}
\end{algorithm}

Each time we update the parameters in the gradient descent algorithm, we are moving by a step of size $\mu$, which is usually called \textit{step-size}. We can show that if $\mu$ is small enough (where small is determined by $\delta$ and $\nu$), then the gradient descent algorithm converges to the minimum of the function. But if we choose a step-size that is too large, then the algorithm may not converge.

If we use a \textbf{constant step-size}, then we can show that for $\mu$ smaller than a certain value (which depends on the Lipschitz constant $\delta$ and the strong-convexity constant $\nu$), the gradient descent converges to the \textbf{exact minimizer}.

%If $\mu$ is greater than a certain value, then the gradient descent 

The algorithm converges at a \textit{geometric} rate on the order $O(\rho^i)$ where $\rho \in (0,1)$ is a decreasing function of the step-size $\mu$ and also depends on $\delta$ and $\nu$.

If use a \textbf{decaying step-size}, then if we consider an iteration-dependent step-size $\mu = \mu(i)$, with $\sum_{i=0}^{+\infty} \mu(i) = +\infty$ and $\lim_{i \to +\infty} \mu(i) = 0$, the gradient descent converges to the \textbf{exact minimizer} with a \textit{non-geometric} rate. In practice, the convergence is still guaranteed but it is slower.

A typical choice is $\mu(i) = \frac{\tau}{i + 1}$ with $\tau > 0$, yielding a convergence rate of $O(\frac{1}{i^{2 \nu \tau}})$.

\subsection{Gradient Descent Limitations}

In practice, we cannot compute the cost function $\E{}{Q_\beta(D)}$ because we do not know the distribution of $D$. However, by using our dataset $\{d_i\}_{i=1}^n$, we can compute the empirical risk:
\[
    J_{emp}(\beta) = \frac{1}{n}\sum_{i=1}^{n} Q_\beta(d_i)
\]

In practice, we do not use this empirical risk because the computation of the gradient:
\begin{itemize}
    \item it is not suited to large datasets, since we need to compute the gradient for each iteration of the algorithm
    \item it is not suited in the case of online learning, since our dataset changes over time and we want to track the evolution of the data
    \item it is not suited for distributed application
\end{itemize}

\section{Stochastic Gradient Descent}
The solution to the limitations of the gradient descent is using an \textbf{instantaneous approximation of the gradient} based on individual samples:
\[
    \hat{\nabla J_i}(\beta) = \nabla Q_\beta(d_i)
\]
We are moving from the expectation of the loss function to the empirical risk by applying the law of large numbers and then we use an approximation of the gradient of this empirical risk based on individual samples. This approximation is called \textbf{stochastic gradient}.

The stochastic gradient descent algorithm is the following:

\begin{algorithm}[H]
    \SetAlgoLined
    Set a starting point $\beta_0$ \\
    \For{$i=1,2,\dots$}{
        take a fresh sample $d_i$ \\
        $\beta_{i} = \beta_{i-1} - \mu \hat{\nabla J_i}(\beta_{i-1})$
    }
    \caption{Stochastic Gradient Descent}
\end{algorithm}

With respect to the gradient descent:
\begin{itemize}
    \item we are using an instantaneous approximation of the gradient based on individual samples that is \textit{noisy} estimate of the true gradient based on the current sample $d_i$, which means we are not considering the information contained in the other samples.
    \item $\beta_i$ is a stochastic process, since it depends on the random samples $d_i$.
    \item We are computing the average \textit{over time} of the gradients of the loss function for each sample.
\end{itemize}
\paragraph*{Constant step-size}
We can show that with constant step-size, the stochastic gradient descent \textit{iterates} (meaning the $\beta_i$) never reach the optimal solution. This is due to the \textit{gradient noise} that is the \textit{inherent} randomness of the gradient. The iterates are \textit{oscillating} around the exact minimizer.


For a $\mu$ smaller than a certain value (which depends on $\delta$ and $\nu$), the iterates $\beta_i$ oscillates in a smaller neighbourhood of the true minimizer $\beta^\ast$.

The error $\E{}{\|\beta_i - \beta^\ast\|^2}$ converges to a steady-state error $O(\mu)$ at a geometric rate $O(\rho^i)$ where $\rho \in (0,1)$ is a decreasing function of the step-size $\mu$ and also depends on $\delta$ and $\nu$.

The more smaller is $\mu$ the more are the data that we are averaging, which means that we are considering more the previous information and less the current information. In practice that the stochastic gradient descent with constant step-size allows us to react to data drifts within a fixed number of samples that it is about $\frac{1}{\mu}$. This means that if we set $\mu = 0.01$ we can react to data drifts within 100 samples.

\paragraph*{Decaying step-size}
We can show that with decaying step-size, the stochastic gradient descent \textit{iterates} (meaning the $\beta_i$) converge to the exact minimizer and the error $\E{}{\|\beta_i - \beta^\ast\|^2}$ converges to zero.

If $\mu(i) = \frac{\tau}{i+1}$ for $\tau > \frac{1}{\nu}$ the convergence rate it $O(\frac{1}{i})$.

\paragraph*{Online learning}
We can say that decaying step-sizes converge to the exact minimizer but they are not suited to online tasks, because the algorithm cannot react to data drifts since the updates are given less importance due to the increasingly smaller values of $\mu(i)$. We can still use it in \textit{batch} applications rather than online tasks.

By contrast, constant step-sizes are suited to online tasks, but they do not converge to the exact minimizer. However, they converge to a neighbourhood of the minimizer and they are able to react to data drifts. The smaller the $\mu$, the better the accuracy of the algorithm, but the slower the convergence.