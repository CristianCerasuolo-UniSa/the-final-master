\chapter{Linear Methods for Regression}
\section{Resampling Methods}
\textbf{Resampling Methods} involve repeatedly drawing elements from the training set and refitting a model of interest on each sample to retrieve additional information about the fitted model. For example, estimates of test-set prediction error and a characterization of parameters estimator.

They can be computationally expensive because the same statistical method is repeated multiple times, using different subsets of the training data set.

These methods are used to perform \textbf{model assessment} and \textbf{model selection}. Model assessment involves quantifying model uncertainty or estimate test error taes, while \textit{model selection} involves selecting the proper level of flexibility for a model, identifying which regressor are used to describe the dependent variable.

\textit{Difference between training error and test error}: the \textbf{test error} is the average error that results from using a statistical learning method to predict the response on a new observation, one that was not used in training the method.

In contrast, the \textbf{training error} can be easily computed by applying the statistical learning method to the observations used in its training.

The training error rate often is quite different from the test error rate. In particular, the training error rate can be \textit{much smaller} than the test error rate, because most statistical methods specifically estimate parameters by minimizing the training error rate.

In general, training error will always decline. However, the test error rate will decline at first but will then start to increase again.

The best solution to estimate the test error is to use a \textit{large} designated test set. However, often we do not have a large enough test set available. Some methods involve adjusting the training error rate to account for the bias by employing a correction factor (\textit{AIC}, \textit{BIC} or the Cp statistic).

Resampling methods instead estimate the test error rate by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations.

\subsubsection*{Validation Set Approach}
In this approach, we randomly divide the available set of samples into two parts: a \textbf{training set} and a \textbf{validation set} or \textbf{hold-out set}. The model is fit on the training set and the fitted model is used to predict the responses for the observations in the validation set.

The resulting validation-set error provides an estimate of the test error. The validation-set error is compute using MSE in the case of a quantitative response, and using the classification error rate in the case of a qualitative response.

\paragraph*{Example}
Suppose that we want to predict \textit{mpg} from \textit{horsepower}. The \texttt{Auto Dataset} is made by $392$ observations. We try to fit a polynomial regression model, with $d$ degree. In order to find which degree gives the best fit, we:
\begin{itemize}
    \item randomly split the data into training and validation data of size $196$ each;
    \item fit the models on the training set using different degree of the polynomial;
    \item evaluate all fitted models using the validation data set;
    \item the model with the lowest validation MSE is selected.
\end{itemize}
Then the final model will have the degree of the model with the lowest validation MSE fitted with all the data.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{./figures/lec_15_validation_set.png}
    \caption{Left: Validation error rate for a single split; Right: Validation error rate for different random splits.}
    \label{fig:lec_15_validation_set}
\end{figure}

As we can see from the plots above, there is a lot of variability among the MSE if we change the training/validation set. The validation set approach has two main drawbacks:
\begin{itemize}
    \item the validation MSE can be highly variable
    \item only a subset of the observations are used to fit the model, reducing the data uset to train the model.
\end{itemize}
The validation set error may tend to \textbf{overestimate} the test error for the model fit. The \textbf{cross-validation} approach overcomes these two drawbacks.

\subsubsection*{Leave-One-Out Cross-Validation}
Split the data ${(x_i, y_i)}$ into a validation set $(x_1,y_2)$ and a training set $(x_2, y_2, \dots, x_n, y_n)$. Fit the model on the training set and validate the model using the validation set, computing the corresponding \textit{test error} MSE. Repeat this procedure $n$ times, each time leaving out a different observation. The LOOCV estimate for the test MSE is the average of these $n$ test error estimates.
\[
    CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n} MSE_i
\]

The LOOCV approach has a number of desirable properties:
\begin{itemize}
    \item it has far less bias than the validation set approach, since we repeatedly fit the model using a training set that contains $n-1$ observations, almost as many as are in the entire data set.
    \item it produces a less variable MSE estimate, since there is no randomness in data splits.
\end{itemize}

The main issues with this method is that it can be computationally expensive, because each model is fit $n$ times.

\subsubsection*{k-Fold Cross-Validation}
The idea behind the k-fold cross.validation is to randomly divide the data into $K$ equal-sized parts. We leave out part $k$, fit the model to the other $K-1$ parts and then obtain predictions for the left-out $k$th part. This is done in turn for each part $k = 1, \dots, K$, and then the results are averaged.

Let the $K$ parts be $C_1, \dots, C_K$, where $C_k$ denotes the indices of the observations in the $k$th part. There are $n_k$ observations in part $k$, if $n$ is a multiple of $K$ then $n_k = \frac{n}{K}$.

We compute:
\[
    CV_{(K)} = \frac{1}{K} \sum_{k=1}^{K} MSE_k = \frac{1}{K} \sum_{k=1}^{K} \frac{1}{n_k} \sum_{i \in C_k} (y_i - \hat{y}_i)^2
\]
Where $MSE_k$ is the mean squared error for the $k$th part, and $\hat{y}_i$ is the prediction obtained from the fit of the model to the $k$th part, excluding observation $i$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{./figures/lec_15_k_fold_cv.png}
    \caption{Left: LOOCV error curve; Right: 10-fold CV error curves.}
    \label{fig:lec_15_k_fold_cv}
\end{figure}

As we can see in the plots above, if we repeat the k-fold cross validation with different $K$ we obtain different MSE, but the variance is lower than the validation set approach. Both the LOOCV and the k-fold CV methods tend to give similar results, they are both stable and produce similar MSE estimates.

\paragraph*{LOOCV vs K-Fold CV}
K-fold CV with $K < n$ has a computational advantage over LOOCV, and gives \textit{more accurate estimates} of the \textit{test error} rate with respect to LOOCV, this is because of the \textbf{bias-variance trade-off}.

\textbf{LOOCV} has \textit{less bias} but \textit{higher variance} than $K$-fold CV. The bias is smaller because LOOCV uses a training dataset containing $n-1$ samples while k-fold uses a training dataset containing $(K-1) \frac{n}{k}$. The variance is higher because LOOCV averages the outputs of $n$ fitted models each of which is trained on an almost identical set of observations, highly correlated with each other, by contrast in K-fold CV we are averaging the outputs of $K$ fitted models that are trained on less correlated sets of observations, because the overlap between the training set is smaller.\footnote{The mean of many highly corrlated quantities has higher variance with respect to the mean of many quantities that are not highly correlated.}

In conclusion, most of the times we use $K$-fold CV with $K=5$ or $K=10$, because it has been shown empirically that it suffer neither from excessively high bias nor from very high variance.

\subsubsection*{Bootstrap}
The \textbf{bootstrap} method is used to quantify the uncertainty associated with a given estimator. For example, it can provide an estimate of the standard error of a coefficient, or a confidence interval for that coefficient. The main advantage is the fact it can be easily applied to a wide range of statistical learning methods, including some for which a measure of variability is otherwise difficult to obtain.

\paragraph*{Example}
Suppose we wish to invest a fixed sum of money in two financial assets that yield returns of $X$ and $Y$, where $X$ and $Y$ are random variables.

We will invest a fraction $\alpha$ of our money in $X$ and will invest the remaining $1-\alpha$ in $Y$. We wish to choose $\alpha$ to minimize the total risk or \textit{variance} of our investment.

The \textbf{variance} of our investment is given by:
\[
    \sigma^2 = \text{Var}(\alpha X + (1-\alpha)Y)
\]
The value of $\alpha$ that minimizes $\sigma^2$ is given by:
\[
    \alpha = \frac{\sigma^2_Y - \sigma_{XY}}{\sigma^2_X + \sigma^2_Y - 2\sigma_{XY}}
\]
The problem is that the variances and covariances are unknown, so we cannot compute $\alpha$. We can compute estimates of these quantities using a dataset that contains past measurements for $X$ and $Y$ and then use these estimates to compute $\hat{\alpha}$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{./figures/lec_15_investment_simulations.png}
    \caption{Each panel displays 100 simulated returns for investments $X$ and $Y$, each of them gives a different estimate of $\alpha$.}
    \label{fig:lec_15_bootstrap_example}
\end{figure}

To estimate the standard deviation of $\hat{\alpha}$ we repeated the process of simulating the data set of 100 paired observations of $X$ and $Y$ many times, each time recomputing $\hat{\alpha}$. This gave us $1000$ estimates for $\alpha$ which we can call $\hat{\alpha}_1, \hat{\alpha}_2, \dots, \hat{\alpha}_{1000}$.

For these simulation the parameters were $\sigma_X^2 = 1$, $\sigma_Y^2 = 1.25$, $\sigma_{XY} = 0.5$ and thus $\alpha=0.6$.

The mean of the $\hat{\alpha}_i$ is $0.5996$ and the standard deviation is $0.083$. Since the standard deviation is an estimate of the standard error of $\hat{\alpha}$, we can say that $0.083$ is almost certainly a good estimate of the standard error of $\hat{\alpha}$.

\paragraph*{Bootstrap}
For real data we cannot generate new samples from the original population. The bootstrap approach allow us to mimic the process of obtaining new data sets, so that we can estimate the variability of our estimate without generating additional samples.

Rather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the original data set with replacement.

Each of these data sets is of the same size as the original data set. As a result, some observations may appear more than once in a given bootstrap data set and some not at all.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{./figures/lec_15_bootstrap.png}
    \caption{Example of bootstrap procedure.}
    \label{fig:lec_15_bootstrap}

\end{figure}

Denoting the first bootstrap data set by $Z^{\ast 1}$, we use it to produce a new bootstrap estimate for $\alpha$, which we call $\hat{\alpha}^{\ast 1}$. This procedure is repeated $B$ times for some large value of $B$, in order to produce $B$ different bootstrap data sets, $Z^{\ast 1}, Z^{\ast 2}, \dots, Z^{\ast B}$, and $B$ corresponding $\alpha$ estimates, $\hat{\alpha}^{\ast 1}, \hat{\alpha}^{\ast 2}, \dots, \hat{\alpha}^{\ast B}$.

The standard error is given by the formula below:
\[
    SE_B(\hat{\alpha}) = \sqrt{\frac{1}{B-1} \sum_{r=1}^{B} \left(\hat{\alpha r} - \frac{1}{B} \sum_{r^{\prime} =1}^{B} \hat{\alpha}^{\ast r^{\prime} }\right)}
\]
This serves as an estimate of the standard error of $\hat{\alpha}$.

In the investement example, we can use the bootstrap to estimate a standard error of $\hat{\alpha}$, which is $0.087$ which is very close to the value of $0.083$ obtained from the simulation.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{./figures/lec_15_bootstrap_2.png}
    \caption{Comparison between bootstrap and simulation.}
    \label{fig:lec_15_bootstrap_2}

\end{figure}

In more complex situations, figuring out the appropriate way to generate bootstrap samples can require some thought. For example, if the data is a \textit{time series} we cannot simply sample the observations with replacement, but we can instead create blocks of consecutive observations and then sample those with replacements. Then we paste together the blocks to obtain a bootstrap data set.

% caption

% didn't understand this part
The bootstrap approach is primarily used to obtain standard errors of an estimate. It also provides approximate confidence intervals for a population parameter, which represents an approximate $90\%$ confidence interval for the true $\alpha$ and it is called \textbf{bootstrap percentile} confidence interval.

It is the simplest method (among many approaches) for obtaining a confidence interval from the bootstrap.

\subsection*{Final Remarks}
If we have many data, the best approach for both problems is to randomly divide the dataset into three parts called \textit{training set}, \textit{validation set} and \textit{test set}.

\begin{itemize}
    \item the \textbf{training set} is used to fit the models
    \item the \textbf{validation set} is used to estimate prediction error and perform model selection
    \item the \textbf{test set} is used for assessment of the generalization error of the final chosen model
\end{itemize}

Ideally, the test set should be brought out only at the end of the data analysis. If instead we used the test-set repeatedly and select the model with the smallest test-set error, we will underestimate the true test error.

Tipically, we use $50\%$ of the data for training, $25\%$ for validation and $25\%$ for testing. However, there are many situation in which there is insufficient data to split it into three parts. In this case, the dataset is typically split in two parts called training and test parts.

In these circumstances, the validation step is approximated either anaylitically using AIC and BIC or by using resampling methods.

\section{Model Selection}
The standard linear model is commonly used to describe the relationship between a response $Y$ and a set of predictors $X_1, \dots, X_p$. This model is typically fit using least squares which may not work well for large $p$ or in presence of multicollinearity.

There are two reasons we might not just use the ordinary least squares estimates, that are \textit{prediction accuracy} and \textit{model interpretability}.

\textbf{Prediction accuracy} decreases when, fixing the number of samples, we increase the number of predictors.

If $n \gg p$, meaning that if the number of observation is much larger than the number of variables, then the least squares estimates tend to also have low variance and will perform well on test observations\dots

If $n \simeq p$, then the least squares fit can have high variance and may result in overfitting and poor estimates on unseen observation.

When $n < p$ then there is no longer a unique least squares coefficient estimate, since the variance is infinite so the method cannot be used at all.

In addition, when we have a large number of predictors $X$ in the model, there will be many that have little or no effect on $Y$, this reduces the \textbf{model interpretability}. Leaving these variables in the model makes it harder to see the real relationships among predictors and the dependent variable and it is difficult to appreciate the effect of the \textit{relevant variables} describing $Y$. The model would be easier to interpret by removing the unimportant variables.

There are three different approaches that we can use to prevent this problems:
\begin{itemize}
    \item \textbf{subset selection}, in which we identify a subset of the $p$ predictors that we believe to be related to the response, we then fit a model using the least squares on the reduced set of variables.
    \item \textbf{shrinkage}, in which we fit a model involving all $p$ predictors but the estimated coefficient get shrunken towards zero. This techniques reduces variance and can perform variable selection.
    \item \textbf{dimension reduction}, in which we project the $p$ predictors into a $M-$dimensional subspace. This is achieved computing $M$ different linear combinations or projections of the variables. This $M$ projects are used as predictors to fit a linear regression model using OLS.
\end{itemize}

There are four different kinds of algorithms that implement the \textbf{subset selection approach}:
\begin{itemize}
    \item \textbf{best subset selection}
    \item \textbf{forward stepwise selection}
    \item \textbf{backward stepwise selection}
    \item \textbf{hybrid approach}
\end{itemize}

\subsection*{Best Subset Selection}
In this approach, we run a linear regression for each possibile combination of the $X$ predictors. In order to select the "best" model, one simple apporach is to take the subset with the smallest \textit{RSS} or equivalently the largest $R^2$.

Unfortunately, the model that includes all the variables will always have the largest $R^2$ and the smallest \textit{RSS}, since these quantities are related to the training error.

The algorithm for best subset selection is the following:
\begin{enumerate}
    \item Let $\mathcal{M}_0$ denote the \textit{null model}, which contains no predictors. This model simply predicts the sample mean for each observation.
    \item For $k = 1,2,\dots, p$:
    \item[(a)] Fit all $\binom{p}{k}$ models that contain exactly $k$ predictors.
    \item[(b)] Pick all the best among these $\binom{p}{k}$ models, and call it $\mathcal{k}$. Here best is defined as having the smallest RSS, or equivalently largest $R^2$.
    \item Select a single best model from among $\mathcal{M}_0, \dots, \mathcal{M}_p$ using cross-validated prediction error, $C_p$, AIC, BIC or adjusted $R^2$.
\end{enumerate}

This algorithm can be used for other types of models not based on least squares regression, such as \textit{logistic regression}.

\textit{Example. }

RSS and $R^2$ are not suitable for selecting the best model among a collection of models with \textit{different} number of predictors.

In order to select the best model with respect to the test error, we need to estimate this test error. There are two possible approaches:
\begin{enumerate}
    \item Estimate test error by making an \textit{adjustement} to the training error to account for the bias due to overfitting. We can use measurements such as: $C_p$, $AIC$, $BIC$, and adjusted $R^2$. These methods add a penalty to RSS for the number of variables in the model.
    \item We can \textit{directly} estimate the test error, using a validation set or a cross-validation approach.
\end{enumerate}

\subsubsection*{Mallow's $C_p$}
For a fitted OLS model containing $d$ predictors, the $C_p$ estimate of test MSE is:
\[
    C_p = \frac{1}{n} \left(RSS + 2d\hat{\sigma}^2\right)
\]
where $\hat{\sigma}^2$ is an estimate of $\sigma^2 = \text{Var}(\epsilon)$. This statistics adds a penality of $2d\hat{\sigma}^2$ to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error.

We can show that if $\hat{\sigma}^2$ is an unbiased estimate of $\sigma^2$, then $C_p$ is an unbiased estimate of test MSE.

\subsubsection*{Akaike Information Criterion}
The AIC criterion is defined for a large class of models fit by maximum likelihood:
\[
    \text{AIC} = -2\log L + 2 \cdot d
\]
where $L$ is the maximized value of likelihood.

In the case of the linear model with Gaussian errors, maximum likelihood and least squares are the same thing. In this case AIC is given by:
\[
    \text{AIC} \propto \text{RSS} + 2d\hat{\sigma}^2
\]
Thus, for least squares models, $C_p$ and AIC are equivalent.

\subsubsection*{Akaike Information Criterion}
The BIC criterion is derived from a Bayesian point of view, but ends up looking similar to $C_p$ and AIC. For the OLS model with $d$ predictors the BIC is given by:
\[
    \text{BIC} = - 2 \log L + \log(n) d
\]
which means:
\[
    BIC \propto RSS + \log (n)d \hat{\sigma}^2
\]

Since $\log n > 2$, for any $n > 7$, the BIC statistic generally places a heavier penalty on models with many variables and hence results in the selection of smaller models than $C_p$.

\subsubsection*{Adjusted $R^2$}
For a least squares model with $d$ variables, the adjusted $R^2$ statistic is calculated as:
\[
    \text{adjusted } R^2 = 1 - \frac{\frac{\text{RSS}}{n-d-1}}{\frac{\text{TSS}}{n-1}}
\]
A large value of adjusted $R^2$ indicates a model with a small test error.
Unlike the other statistics presented, for which a small value indicates a model with a low test error, a large value of adjusted $R^2$ indicates a model with a small test error.

Maximizing the adjusted $R^2$ is equivalent to minimizing $\frac{\text{RSS}}{n-d-1}$. While RSS always decreases as the number of variables in the model increases, $\frac{RSS}{n-d-1}$ may increase or decrease, due to the presence of $d$ in the denominator.

Unlike the $R^2$ statistic, the adjusted $R^2$ statistic pays a price for the inclusion of unnecessary variables in the model.

$C_p$, AIC, BIC have all rigorous theoretical justifications. The adjusted $R^2$ is quite intuitive but it's not motivated in statistical theory. All of these measures are simple to compute and can serve estimates of the test error but none of then is perfect. They can also be compute in case of general types of models.

\subsubsection*{Validation and cross-validation}

Each of the procedures returns a sequence of models $\mathcal{M}_k$ indexed by model size $k = 0,1,2,\dots$. We need to select the best $\hat{k}$.
We then compute the validation set error or the cross-validation error for each model $\mathcal{M}_k$ under consideration and the select the $k$ for which the resulting estimated test error is smallest.

With respect to the previous estimates, the validation set approach provides a direct estimate of the test error without requiring the variance $\sigma^2$. This approach can also be used in a wider range of model selection tasks.

\paragraph*{One-standard-error rule}
We usually see that the model with the lowest estimated test error has a certain number of predictors, but there isn't much difference in terms of test errors between the best model and some models with less predictors.

We can select a model using the \textbf{on-standard-error rule}, we calculate the standard error of the estimated test \textit{MSE} for each model size and then we select the smallest model for which the estimated test error is within one-standard error of the lowest point on the curve.

\subsubsection*{Stepwise selection}
Best subset selection suffer from computational limitations, meaning that it can work for at most $p$ as large as $30$ or $40$ predictors.
Best subset selection may also suffer from statistical problems when $p$ is large, because the larger is the search space, the higher the change of finding models that perform well on the training data. An enourmous search space can lead to overfitting and high variance of the coefficient estimates.

So we usually adopt \textit{stepwise methods}.

\paragraph*{Forward Stepwise Selection}

\section{Shrinkage Methods}
The subset selection methods use least squares to fit a linear model that contains a subset of the predictors. As an alternative we can fit a model containing all \textit{p} predictors using a technique called \textbf{shrinkage} that \textit{constraints} or \textit{regularize} the coefficient estimates towards zero.

This technique can significantly reduce the variance of the coefficients estimate at the cost of increasing the bias error.

\subsection*{Ridge Regression}
Given our training data $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n $ ordinary least squares minimzes the \textit{residual sum of squares}:
\[
    \text{RSS} = \sum_{i=1}^{n} \left(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}\right)^2
\]
On the other hand we can use \textit{ridge regression} that minimizes the following equation:
\[
    \text{RSS'} = \sum_{i=1}^{n} \left(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}\right)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
\]
where $\lambda$ is a tuning parameter. This additional term is called \textbf{shrinkage penalty}.

The shrinkage penalty can shirnk the estimates of $\beta_j$ towards zero. The tuning paramter $\lambda$ is used to enforce or reduce the shrinkage. We will use $\hat{\beta}^R$ to denote the coefficient estimates of the parameters $\beta$ for a fixed value of $\lambda$. When $\lambda = 0$, we get the OLS estimates, whereas when $\lambda \to \infty$ the impact of the penalty term grows and the coefficient estimates will approach zero.

\textbf{Note:} the shrinkage penalty is not applied to the intercept parameter $\beta_0$, becase the intercept is a \textbf{measure of the mean value of the response} when independent variables are equal to zero.

The first plot shows the standardized ridge regression coefficients with $\lambda$ as the independent variable, while the second plot shows the coefficients with the term $\frac{\lVert \hat{\beta}_\lambda^R \rVert_2}{ \lVert \hat{\beta} \rVert_2}$ as the independent variable. This term is the ratio between the $\ell_2$ norm of the ridge regression estimates and the OLS estimates. This quantity ranges from $1$, when $\lambda = 0$ to $0$, when $\lambda$ approaches infinity. If this quantity is small, it means that the ridge regression coefficient estimates have been shrunken very close to zero.

\paragraph*{Scaling}
The OLS estimates are \textit{scale invariant} or \textit{scale equivariant}, which means that the we can fit the model even if the regressors have different scaling. This happens because multiplying $X_j$ by a constant $c$ means scaling the coefficient estimates by a factor of $\frac{1}{c}$, thus having the same result.

This is not true for the ridge regression estimates that can change substantially when multiplying a given predictor by a constant. The term $X_j \hat{\beta}_{j,\lambda}^R$ will depend not only on the value of $\lambda$ but also on the scaling of the $j$-th predictor and even the scaling of other predictors.

It is strongly advised to standardize the predictors before applying ridge regression:
\[
    \tilde{x}_{ij} = \frac{x_{ij} - \overline{x}_j}{s_j}
\]
Where $\overline{x}_j$ is the arithmetic mean and $s_j^2$ is the \textit{sampling variance}.

\paragraph*{Why does ridge regression works?}
The penalty term makes the ridge regression estimates biased, because \textbf{???} but can also reduce variance, since that term can be minimized only by
shrinking the coefficient estimates.

The first plot shows the error components with $\lambda$ as an independent variable. We can see that the bias increases as $\lambda$ increases while the variance decreases as $\lambda$ increases, so we can observe a point of minimum in the test mean squared error curve, that indicates the best value of lambda for that model.

Ridge regression will perform better than OLS in situations where the OLS have high variance, such as $p \simeq n$, $p > n$ or multicollinearity. It is also more efficient than OLS because the computations required to estimate ridge regression for all values of $\lambda$ are almost identical to those for fitting a model using OLS and BSS.

If we find out during our analysis that the best value for $\lambda$ is close to zero, then ridge regression is not improving our estimates with respect to OLS.

\subsection*{Least Absolute Shrinkage and Selection Operator}
Ridge regression does not highlights the best predictors, but it moves the less significant coefficients towards zero. Thus, this approach cannot be used to perform \textit{subset selection}. The lasso approach overcome this disadvantage.

The lasso coefficients, which we will denote with $\hat{\beta}_\lambda^L$, minimize the following quantity:
\[
    \text{RSS'} = \sum_{i=1}^{n} \left(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}\right)^2 + \lambda \sum_{j=1}^{p} |\beta_j |
\]

The penalty term in this case is based $\ell_1$ norm of the parameter vectors, in contrast with the $\ell_2$ norm of the ridge regression. As with ridge regression, the lasso regression shirnks the coefficient estimates towards zero because of the positive penalty term introduced. However, the $\ell_1$ penalty force some coefficient estimates to be exactly equal to zero when $\lambda$ is large enough. It is said that lasso yields \textit{sparse} models, that is models that involve only a subset of the variables.

In order to select a good value of $\lambda$, we can employ the cross-validation approach.

The first plot shows the standardized lasso regression coefficients; here we can see that instead of converging to zero, many of the coefficients are exactly zero and this enables for model selection. In the final model we will remove the predictors that are zero.

In operational research, the minimization problems of the ridge and lasso regression are also called \textbf{dual problems}. We can get an equivalent formulation for this problem called \textbf{primal problem}. For ridge regression the formulation requires to \textit{minimize} the following:
\[
    \text{RSS} = \sum_{i=1}^{n} \left(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}\right)^2 \qquad \text{subject to } \sum_{j=1}^p \beta_j^2 \leq s
\]
For lasso is to minimize the following:
\[
    \text{RSS} = \sum_{i=1}^{n} \left(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}\right)^2 \qquad \text{subject to } \sum_{j=1}^p |\beta_j| \leq s
\]
And there is also an alternative formulation for the \textit{best subset selection approach}, that is to minimze the following:
\[
    \text{RSS} = \sum_{i=1}^{n} \left(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}\right)^2 \qquad \text{subject to } \sum_{j=1}^p I(\beta_j \neq 0) \leq s
\]
The last formulation requires us to find a set of estimates such that RSS is minimized, but with the constraint that no more than $s$ coefficient can be nonzero.

The following plots will show intuitively why lasso regression can perform variable selection. The plot shows the \textit{contour plot} of the RSS in case of a model with two predictors. The first plot shows the constraint region for the lasso approach, while the second shows the constraint region for the ridge regression. On each plot the minimum of the RSS is marked, but we can see that it is outside of the constraint regions. When solving the optimization problems proposed before, we need to find the point associated to the smaller value of the RSS that lies inside the constraint regions.

As for why the constraint regions are shaped like that, it is due to the fact that if we expand the $\ell_2$ norm, we will have the equation $\beta_1^2 + \beta_2^2 \leq s$ that describes the area inside a circumpherence; while if we expand the $\ell_1$ norm, we will have the equation $|\beta_1| + |\beta_2| \leq s$ that describes the area inside the four segments that form a rectangular shape.

Due to the topology of the $\ell_1$ norm, it is more likely that the minimum for a coefficient estimate will be zero, because that topology includes also the corners of the rectangular shape.

In order to compare lasso and ridge regression, let's consider the case in which we have $p=45$ and $n=50$. The first plot shows the mean squared error and its components for lasso regression. We can see that the behaviour is similar to that of the ridge regression but for large values of $\lambda$, the mean squared error converges to a fixed value that is because \textbf{??}.

The second plot shows a comparison between lasso and ridge in terms of mean squared error, with $R^2$ as independent variable. We can see that while the bias curves are almost identical, the variance of ridge is slightly lower than the variance of lasso and this makes ridge regression better than lasso in this particular case.

This happened because all the 45 predictors were truly related to the response, and none of the true coefficients were actually equal to zero. Let us now consider the case where the response is actually a function of only $2$ out of $45$ predictors. The second plot shows that the lasso is way better than ridge regression for every component of the MSE.

From this, we can derive that there is no best approach between ridge or lasso regression, since lasso tends to perform better in a setting only a small number of predictors have substantial coefficients; while ridge regression will perform better when the response is a function of many predictors. But since we don't have this information \textit{a priori} in real cases, we can use the cross validation approach to determine which approach is better on a particular dataset. Additionally, an advantage of lasso is the fact that the models are sparse and thus easier to interpret.


If $X$ is orthonormal, the three procedures have explicit solutions. Let $\hat{\beta}_j$ be the OLS estimate of $\beta_j$.

The best subset of size $s$ is:
\[
    \hat{\beta}_j^B = \hat{\beta}_j \cdot I(|\hat{\beta}_j| \geq |\hat{\beta}_{(s)}|)
\]
where $|\hat{\beta}_{(s)}|$ is the $s$th largest among all $|\hat{\beta}_j|$.

For ridge regression, we have that:
\[
    \hat{\beta}^{ridge} = \left(X^T X + \lambda I\right)^{-1} X^T y = \hat{\beta}_j^R = \frac{\hat{\beta}_j}{1 + \lambda}
\]

And finally for lasso we have that:
\[
    \hat{\beta}_j^L = \text{sign}(\hat{\beta}_j)\left(|\hat{\beta}_j| - \frac{\lambda }{2}\right)_+
\]
where $x_+$ equals $x$ if $x>0$ and equals $0$ if $x\leq 0$.

This allows us to make another comparison between ridge regression, lasso and the best subset selection approach:
\begin{itemize}
    \item ridge regression does a \textbf{proportional shrinkage} of OLS estimates of a factor of $1+\lambda$
    \item lasso translates each coefficient by a constant factor $\frac{\lambda}{2}$, truncating at zero, so it performs a \textbf{soft-thresholding}.
    \item best subset selection drops all variables with coefficients smaller than the $s$-th largest, this is a form of \textbf{hard-thresholding}.
\end{itemize}
% slide 20 plots

\paragraph*{Proposition}
The coefficients of the ridge regression are:
\[
    \hat{\beta}^{ridge} = \left(X^T X + \lambda I\right)^{-1} X^T y
\]
and they can be rewritten as:
\[
    \hat{\beta}_j^R = \frac{\hat{\beta}_j}{1 + \lambda}
\]


\paragraph*{Proof}
We start by solving the following problem:
\[
    \hat{\beta}_{ridge} = \arg\min_\beta  \left\{ \sum_{i=1}^n (y_i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij})^2  + \lambda\sum_{j = 1}^p \beta_j^2 \right\} = \arg\min_\beta \text{RSS}^{\prime} (\beta)
\]
Firstly we need to standardize the data points:
\[
    x_{ij} \to \frac{x_{ij} - \overline{x}_j}{\sigma_j}
\]
Note that $\beta_0 = \overline{y}$. So we consider $y_i - \overline{y}$. Instead of having a design matrix with $p+1$ columns, we use $p$ columns by dropping $\beta_0$.

Then we need to find $\hat{\beta}_{ridge}$ by minimizing $\text{RSS}'(\beta, \lambda)$. We now compute the gradient of $\text{RSS}^{\prime}$ wrt $\beta$ and then equate it to zero.
\[
    \nabla_\beta \text{RSS}^{\prime} (\beta, \lambda) = 0
\]
By considering standardization we have the following dimensions:
\begin{align*}
    X: n\times p     \\
    \beta: p\times 1 \\
    Y: n \times 1
\end{align*}
We now rewrite $\text{RSS}^{\prime}$ in matrix form:
\begin{align*}
    \text{RSS}^{\prime} & = (Y - X \beta)^T (Y - X \beta) + \lambda \beta^T \beta =                                                                                              \\
                        & = Y^T Y - \underbrace{\beta^T X^T Y}_{scalar} - \underbrace{Y^T X \beta}_{scalar}+                 \beta^T X^T X \beta + \lambda I_{p} \beta^T \beta = \\
                        & = Y^T Y - 2 \beta^T X^T Y + \beta^T (X^T X + \lambda I_p) \beta
\end{align*}

Now we can compute $\nabla_\beta \text{RSS}^{\prime} $. The derivative of $Y^T Y$ is zero, the derivative of $- 2 \beta^T X^T Y $ is $- 2 X^T Y $ and finally the derivative of $\beta^T (X^T X + \lambda I_p) \beta$ (because we can show that the matrix inside the brackes is a symmetric matrix) is
$2(X^T X + \lambda I_p) \beta$ so we get:
\[
    \nabla_\beta\text{RSS}^{\prime} = - 2 \beta^T X^T Y + 2(X^T T + \lambda I_p) \beta = 0
\]
The solution of this equation is:
\[
    \hat{\beta}_{ridge} = (X^T X + \lambda I)^{-1} X^T Y
\]
Let us now consider the $\hat{\beta}_{OLS}$ estimator:
\[
    \hat{\beta}_{OLS} = (X^T X)^{-1} X^T Y
\]
The presence of a positive quantity on the primary diagonal lets us invert the matrix, because the determinant of this matrix increases thanks to the positive quantity. This solves the problem of \textit{collinearity}\dots

In case of \textbf{orthonormal inputs} ($n \geq p$):
\[
    X^T X = I_p \qquad \text{condition of orthonormality}
\]

In algebra, if $n = p$ the condition of orthonormality $X^T = X^{-1}$; while if $n > p$ the condition is $X^T X = I_p$ (\textit{pseudoinverse}).
Let's compare again OLS
\[
    \hat{\beta}_{OLS} = (X^T X)^{-1} X^T Y = I_p^{-1} X^T Y = X^T Y
\]
while Ridge:
\[
    \hat{\beta}_{ridge} = (I + \lambda I)^{-1} X^T Y = \frac{I}{1+\lambda} \hat{\beta}_{OLS}
\]
The same approach works also for lasso.
Note that we can rewrite:
\[
    (X^T T + \lambda I_p) = (X^T X)^T + \lambda  I_p^T  = X^T X + \lambda I_p
\]