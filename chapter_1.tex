\chapter{Fundamentals}
\section{Linear Algebra}
\textbf{Matrix dot product}: Assume we have $A \in \mathbb{R}^{m \times n}$, $B \in \mathbb{R}^{n \times p}$, then $C = A B \in \mathbb{R}^{m \times p}$ is defined as:
\[
    C_{ij} = \sum_{k = 1}^{n} A_{ik} B_{kj}
\]

\textbf{Orthogonality}, we say that the matrix $U$ is \textbf{orthogonal} if $U^T U = I$.

\textbf{Similarity}, we say that two matrices $B$ and $C$ are \textbf{similar} if there exists a matrix $P$ such that $B = P^{-1} C P$. When two matrices are similar they have the same eigenvalues.

\textbf{Eigen-decomposition}: From the \textbf{spectral theorem} we know that if $A$ is a real square symmetric matrix then it can be decomposed as $A = U \Lambda U^T$ where $U$ is a matrix obtained by stacking the eigenvectors of $A$ while $\Lambda$ is a diagonal matrix with the eigenvalues of $A$. In other words, if $A$ is a real square symmetric matrix then it is \textbf{similar} to the diagonal matrix $\Lambda$.

In general, the relationship between the eigenvectors and the eigenvalues is the following: $A u^{(k)} = \lambda_k u^{(k)}$ where $u$ is an eigenvector and $\lambda$ is the corresponding eigenvalue.

\textbf{Gradient rules}
\begin{enumerate}
    \item $\nabla_x a^t x = \nabla_x x^T a =  a$
    \item $\nabla_x x^T A x = A^t x + A x$
\end{enumerate}

\section{Law of Large Numbers}

\section{Bayes's Rule}
\begin{theorem}
    Bayes's rule states that:
    \[
        p(x|y) = \frac{p(y|x) p(x)}{p(y)}
    \]
\end{theorem}
\begin{proof}
    By definition of conditional probability we have:
    \[
        p(x|y) = \frac{p(x,y)}{p(y)} \to p(x,y) = p(x|y) p(y)
    \]
    and
    \[
        p(y|x) = \frac{p(x,y)}{p(x)} \to p(x,y) = p(y|x) p(x)
    \]
    \[
        p(x|y) p(y) = p(y|x) p(x) \to p(x|y) = \frac{p(y|x) p(x)}{p(y)}
    \]
\end{proof}

\section{Fundamental theorem of expectation}
Assume if have a discrete random variable $Y$ and a function $h(Y)$ and I want to compute the expected value of $h(Y)$. By definition of expected value, by fixing $Z = h(Y)$, we have:
\[
    \E{}{Z} = \sum_{z \in Z} z p(z) = \sum_{y \in Y} h(y) p(z)
\]
The fundamental theorem of expectation states that we can compute the expected value of $h(Y)$:
\[
    \E{}{h(Y)} = \sum_{y \in Y} h(y) p(y)
\]

\section{Tower Property}
Suppose we have two discrete random variables $X$ and $Y$ and $h$ that is a function of $X$ and $Y$.
\begin{theorem}
    The \textbf{tower property} states that we can compute the expected value of $h(X,Y)$ by first computing the conditional expected value of $h(X,Y)$ given $X$ and then computing the expected value of the result over $X$.
    \[
        \E{Y}{h(X,Y)} = \E{X}{\E{Y}{h(X,Y) \mid X}}
    \]
\end{theorem}

\begin{proof}
    By definition of mean of discrete random variables:
    \[
        \E{Y}{h(X,Y)} = \sum_{x \in X} \sum_{y \in Y} h(x,y) p(x,y)
    \]
    By apply Bayes's rule, we can rewrite $p(x,y)$:
    \[
        \sum_{x \in X} \sum_{y \in Y} h(x,y) p(x,y) = \sum_{x \in X} \sum_{y \in Y} h(x,y) p(y|x) p(x)
    \]
    Since $p(x)$ does not depend on the second summation term, we can rewrite it outside the summation term:
    \[
        \sum_{x \in X}  p(x) \sum_{y \in Y} h(x,y) p(y|x)
    \]
    We can observe that now the second summation term is by definition the conditional mean of $h(X,Y)$:
    \[
        \sum_{x \in X}  p(x) \E{Y}{h(X,Y) \mid X = x}
    \]
    Finally, we can observe that this term is the expected value over $X$ of the conditional mean.
    \[
        \E{X}{\E{Y}{h(X,Y) \mid X = x} }
    \]
\end{proof}





