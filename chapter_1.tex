\chapter{Fundamentals}
\section{Linear Algebra}
\textbf{Matrix dot product}: Assume we have $A \in \mathbb{R}^{m \times n}$, $B \in \mathbb{R}^{n \times p}$, then $C = A B \in \mathbb{R}^{m \times p}$ is defined as:
\[
    C_{ij} = \sum_{k = 1}^{n} A_{ik} B_{kj}
\]

\textbf{Orthogonality}, we say that the matrix $U$ is \textbf{orthogonal} if $U^T U = I$.

\textbf{Similarity}, we say that two matrices $B$ and $C$ are \textbf{similar} if there exists a matrix $P$ such that $B = P^{-1} C P$. When two matrices are similar they have the same eigenvalues.

\textbf{Eigen-decomposition}: From the \textbf{spectral theorem} we know that if $A$ is a real square symmetric matrix then it can be decomposed as $A = U \Lambda U^T$ where $U$ is a matrix obtained by stacking the eigenvectors of $A$ while $\Lambda$ is a diagonal matrix with the eigenvalues of $A$. In other words, if $A$ is a real square symmetric matrix then it is \textbf{similar} to the diagonal matrix $\Lambda$.

In general, the relationship between the eigenvectors and the eigenvalues is the following: $A u^{(k)} = \lambda_k u^{(k)}$ where $u$ is an eigenvector and $\lambda$ is the corresponding eigenvalue.

\section{Law of Large Numbers}

\section{Bayes's Rule}

\section{Theorem of the total expectation}

\section{Tower Property}





