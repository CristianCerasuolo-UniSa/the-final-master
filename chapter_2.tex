\chapter{Estimation Theory}
\section{Introduction to Data Analysis}
\subsection{Data Analysis}
Data Analysis is the process of extracting information from data. Although data and information are often used interchangeably, they are not the same. Data is something that should cointain information, but it is not information itself. In order to extract information from data, we need to build a learning system.

An important thing to know is that when the learning system is \textit{good}, then by increasing the amount of data we have, the available information cannot decrease. In practice, most of the times we do not have a good system, so it can happen that the data fed to the system is misleading and the information we get from the analysis decreases.

The two main families of problems in data analysis are \textit{estimation} and \textit{classification}. The main difference is that in the estimation problem,
we have a set of data and we want to estimate a real value, while in the classi-
fication problem the output is contained in a finite set.

The input of the learning system can be anything (a vector, a matrix, a graph, a sequence,
etc.), but the output is always a real number or a finite set of values. In general,
we do not care about the dimension of the output, because we can always repeat
the problem as many times as the dimension of the output.

Another thing to make clear is that in statistical learning, we do not have a temporal correlation between the variables. When we say that two variables depend on each other, we just mean that they are correlated, without implying the causality.

% qualcosa sul fatto che esiste model based analysis e supervised parametric e non parametric

\section{Bayes Estimation}
Assume we have a random variable $Y$ and we want to approximate it with a single deterministic value. To do that, we need to find the number that, on average, is the closest to the values of the random variable. In other words, we want to find the number $z^\ast$ that minimizes the expected value of the squared \textit{distance} between $Y$ and $z$:
\[
    z^\ast \colon \arg \min_{z \in \mathbb{R}} \E{}{(Y-z)^2}
\]
The quantity we want to minimize is called \textit{mean squared error} (MSE) and in this case is a function of the value $z$. We can find the minimum of this function by computing its derivative and setting it to zero:

\section{Maximum Likelihood Estimation}

\section{Exercises}
\subsection{Maximum Likelihood}

\subsection{MMSE}
\begin{exercise}
    We have a random variable $Y \sim N(0, \sigma^2_y)$ and data
    \[
        X_i = Y_i + W_i \qquad i = 1, \dots, n
    \]
    where variables $W_i$ are \textit{i.i.d} and also independent with respect to $Y$ and distributed according to $W \sim N(0, \sigma_w^2)$.

    Compute the \textit{minimum mean square error estimator}.
\end{exercise}

The MMSE is the posterior mean of $Y$ given $X$:
\[
    \hat{Y} = \mathbb{E}\left[ Y | X\right]
\]
To compute the posterior mean, we need to compute the posterior distribution of $Y$ given $X$. We can do that either by definition or by applying Bayes' Theorem:
\[
    f(y|x) = \frac{\pi(y) \ell(x|y)}{p(x)}
\]


We can infer that:
\[
    f(x_i|y) = \ell(x_i|y) = \frac{1}{\sqrt{2\pi \sigma^2_w}} e^{\frac{-(x-y)^2}{2\sigma_w^2}}
\]
and consequently
\[
    \ell(x|y) = \prod_{i = 1}^{N} \ell(x_i, y)
\]
While we can say that given $Y=y$ then $X_i$ is distributed according to $W_i$ that is shifted by an amount of $y$, because $X_i = Y_i + W_i$ and $f(w|y) = f(w)$, we cannot compute $f(y|x)$ observing that $Y_i = X_i - W_i$ because $X_i$ and $W_i$ are not independent on each other but they are dependent because $X_i = Y_i + W_i$. Thus we need to apply Bayes' Theorem to find out $\ell(y|x)$.

Observe that since we are computing $x$ \textit{given} $y$, then we're actually trying to understand the \textbf{generative mechanism} that produces the data $x$ given the true value $y$. Suppose for example that we're sampling measurements $x$ of the temperature in a room. We're actually measuring the true value of the temperature and some noise $w$ added to it. We can try to estimate the generative mechanism to understand what is the distribution of temperature given the true value.

Recall that, in the Bayesian setting, $f(y|x)$ is called \textit{posterior} function and $\pi(y)$ is called the \textit{prior} function. The posterior function does not tell us what is the generative mechanism of the data but it estimates how $y$ is hidden from the data. Also, we will observe that the \textbf{MMSE} estimator is obtained by combination of likelihood and prior information.

In order to compute $f(y|x)$ we can apply the Bayesian Theorem:
\[
    f(y|x) = \frac{\pi(y) f(x|y)}{p(x)}
\]
Observe that since $f(y|x)$ is a probability density function with respect to $y$, the term $p(x)$ must be a constant with respect to $y$. We can express $p(x)$ as the joint or \textit{marginal} distribution of $x$ and $y$ because
\[
    p(x) = \int \pi(y^{\prime}) \ell(x|y^{\prime} ) dy
\]
Because if we apply to the previous expression the integral to both members, we obtain:
\[
    \int f(y|x) = \frac{\int \pi(y) \ell(x|y) dy}{p(x)} = 1 ???
\]

So, given that $p(x)$ is a constant with respect to $y$, we arrive at the fundamental proposition that:
\[
    f(y|x) \propto \pi(y) \ell(x|y)
\]
It is propotional because I divide by $p(x)$ that is a factor that is completely determined by $y$. If $p(x)$ is not a constant given $y$ then $f(y|x)$ is not a probability density function.

Some remarks:
\begin{itemize}
    \item In this part of the course we're assuming that we already know the model, so $\pi(y)$ and $\ell(x|y)$ are known.
    \item If our data follows the model, then no other algorithm (not even deep learning) can outperform the estimators derived by our models
    \item We do not create a generative mechanism but we "pretend" to know it.
\end{itemize}

To do a practical example, we can consider a weather forecasting system, with input data that is temperature, humidity and pressure and output data that is if tomorrow will be sunny or rainy. The \textit{posterior} function (likelihood function) tells us if given that is sunny or rainy what could be the probabile values of the sensor data. The \textit{prior} function tells us on average if it is rainy or sunny.

Now we're going to calculate $f(y|x)$. We've said that apart from a constant $p(x)$, that is constant with respect to $y$ but a function of $x$, our \textit{posterior} function is:
\[
    f(y|x) \propto \pi(y) \ell(x|y)
\]
We know that:
\[
    \ell(x|y) =  \prod_{i = 1}^{N} \frac{1}{\sqrt{2\pi \sigma^2_w}} e^{\frac{-(x_i-y)^2}{2\sigma_w^2}}
\]
and that
\[
    \pi(y) = \frac{1}{\sqrt{2\pi\sigma_y^2}} e^{-\frac{y^2}{2\sigma_y^2}}
\]
So we can write:
\[
    f(y|x) \propto \pi(y) \ell(x|y) = \frac{1}{\sqrt{2\pi\sigma_y^2}} \frac{1}{(2\pi\sigma_w^2)^{\frac{N}{2}}} e^{-\frac{y^2}{2\sigma_y^2}} e^{-\frac{1}{2\sigma_w^2} \sum_{i=1}^{N} (x_i-y)^2}
\]
We can ignore the constants:
\[
    f(y|x) \propto e^{-\frac{y^2}{2\sigma^2_y} - \frac{1}{2\sigma_w^2} \sum_{i=1}^{N} x_i^2 - \frac{N}{2} \frac{y^2}{2 \sigma_w^2} + \frac{y}{\sigma_w^2} \sum_{i=1}^{N} x_i}
\]
Since the second term of the exponential is costant with respect to $y$ and it can be written as a product, it can be ignored because we're considering the fact that is proportional. After ignoring the constant, we can observe that in the following expression the first term depends on the \textit{prior} function while the second term depends on the \textit{posterior} function.
\[
    f(y|x) \propto e^{-\frac{y^2}{2\sigma^2_y} - \frac{N}{2} \frac{y^2}{2 \sigma_w^2} + \frac{y}{\sigma_w^2} \sum_{i=1}^{N} x_i}
\]
We can factor the terms that depend on $y^2$:
\[
    f(y|x) \propto e^{-\frac{y^2}{2}\left(\frac{1}{\sigma_y^2} + \frac{1}{\frac{\sigma^2_w}{N}}\right) + \frac{y}{\sigma_w^2} \sum_{i=1}^{N} x_i}
\]
and then by defining:
\[
    \frac{1}{\sigma^2} = \frac{1}{\sigma_y^2} + \frac{1}{\frac{\sigma^2_w}{N}}
\]
We get:
\[
    f(y|x) \propto e^{-\frac{1}{2\sigma^2} y^2 + \frac{\sum x_i}{\sigma_w^2} y}
\]
Now we try to complete the square by adding and subtracting a function $g(x)$:
\[
    f(y|x) \propto e^{\frac{-(y-\mu)^2}{2\sigma^2}}
\]
where:
\[
    \mu = \frac{\sigma^2}{\sigma_w^2}\sum_{i=1}^{N} x_i
\]
So we obtained that the posterior mean is $\mu$:
\[
    \hat{Y} = \mathbb{E}\left[ Y | X\right] = \frac{\sigma_y^2}{N\sigma_y^2 + \sigma_w^2} \sum_{i=1}^{N} x_i
\]



