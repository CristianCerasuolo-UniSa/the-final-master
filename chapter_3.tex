\chapter{Regression}
\section{Model Based Regression}
In this section we will study model-based regression, which means the models of the data and the error are given and we are only interested in finding the \textit{regression function}, also called \textit{optimal estimator}.

Suppose we have $Y \in \mathbb{R}$ (that is our parameter of interest) and data $X \in \mathbb{R}^d$, both random variables.

\begin{definition}
    The optimal regression function is the mean of the posterior distribution of the target variable given the data, i.e. the \textbf{MMSE}:
    \[
        r(X) = \E{}{Y\mid X}
    \]
\end{definition}

\begin{exercise}
    Compute the regression function for the multiple linear regression model:
    \[
        Y = \beta_0 + \sum_{i=1}^{d} \beta_i X_i + \mathcal{E}
    \]
    where $Y$ is the response variable, $\beta_0$ is the intercept term, $\beta_i$ are the coefficients of the regressors $X_i$ and $\mathcal{E}$ is the error term.

    The common assumption we do in multiple linear regression is that $\mathcal{E}$ is a zero mean random variable with variance $\sigma^2$.

    Let us rewrite the model in matrix form:
    \[
        Y   = \beta^T X + \mathcal{E}
    \]
    where $\beta = (\beta_0, \beta_1, \dots, \beta_d)^T$ is the \textit{parameters} vector and $X = (1, X_1, \dots, X_d)^T$ is the \textit{predictors} vector.\footnote{We can write this both as $\beta^T X$ or $X^T \beta$ because the final product is a scalar thus the order of the product doesn't matter, but we will use the first notation.}

    Now we can compute the regression function:
    \[
        r(X) = \E{}{Y \mid X} = \E{}{\beta^T X + \mathcal{E} \mid X}
    \]
    By applying the linearity properties of the expectation:
    \[
        r(X) = \beta^T \E{}{X \mid X}  + \E{}{\mathcal{E} \mid X}
    \]
    The first expectation $\E{}{X \mid X}$ is $X$ because we are conditioning on $X$ and $X$ is a constant given $X$, while the second expectation $\E{}{\mathcal{E} \mid X}$ is zero because in this setting we need to assume that $\mathcal{E}$ is conditionally zero mean given $X$.

    In conclusion, we found that the optimal regression function for the multiple linear regression model is:
    \[
        r(X) = \beta^T X
    \]

\end{exercise}
\begin{theorem}
    Assuming a general regression problem, i.e. $Y_0 = r(X_0) + \mathcal{E}$, the error is conditionally zero mean:
    \[
        \E{}{\mathcal{E} \mid X} = 0
    \]
\end{theorem}

\begin{proof}
    The error is defined as following:
    \[
        \mathcal{E} = Y_0 - r(X_0)
    \]
    We can compute the conditional expectation of the error given $X$:
    \[
        \E{}{\mathcal{E} \mid X_0} = \E{}{Y_0 - r(X_0) \mid X_0} = \E{}{Y_0 \mid X_0} - r(X_0)
    \]
    The first term is the conditional expectation of $Y_0$ given $X_0$, which is equal to $r(X_0)$ by definition of regression function. So we get the following:
    \[
        \E{}{\mathcal{E} \mid X_0} = r(X_0) - r(X_0) = 0
    \]
\end{proof}

Note that if we assumed the shape of optimal function, like we did in the linear regression case, we need to make the assumption that the error is conditionally zero mean. If this assumption wasn't true, it would mean that we made a wrong assumption on the shape of the optimal function.
\[
    \E{}{\mathcal{E} \mid X_0} = r(X_0) - \beta^T X_0 \neq 0 \implies r(X_0) \neq \beta^T X_0
\]

\begin{exercise}
    Find the relationship between $\beta$ and $X$ and $Y$. In particular, we want to find an expression for $\beta$ that depends on some moments of $X,Y$ and $XY$.

    Assume we are working with the multiple linear regression model:
    \[
        Y = X^T \beta + \mathcal{E}
    \]
    where $Y \in \mathbb{R}$, $X \in \mathbb{R}^{(d+1) \times 1}$, $\beta \in \mathbb{R}^{(d+1) \times 1}$ and $\mathcal{E} \in \mathbb{R}$.

    First, since tipically the vector relation $a^Tb$ is not invertible, we multiply on the left both members by $X$ in order to create a matricial relation:
    \[
        \underset{(d+1) \times 1}{XY} = \underset{(d+1)\times 1}{X X^T\beta} + \underset{(d+1)\times 1}{X\mathcal{E}}
    \]
    Then we take expectation, to remove the unknown error term, and we get the following relation:
    \[
        \E{}{XY} = \E{}{X X^T} \beta + \E{}{X \varepsilon} \label{eq:1} \tag{1}
    \]
    And we compute each term separately.

    For the first term, we can observe that the parameter vector is a constant with respect to $X$ so we can take it out of the expectation. To compute the expectation of the matrix $X^T X$, let's consider a generic entry:
    \[
        [X X^T]_{ij} = X_i X_j \to \E{}{[X X^T]_{ij}} = \E{}{X_i X_j}
    \]
    Since the expected value between $X_i X_j$ is a correlation, $XX^T$ is a quantity that describes the correlation between the data.
    \begin{definition}
        The matrix $R_X \triangleq \E{}{X X^T}$ is the \textbf{(auto)-correlation matrix}.
    \end{definition}
    The same reasoning can be applied to $\E{}{XY}$, where we get a corretlation between the data and the response variabile.
    \begin{definition}
        The matrix $R_{XY} \triangleq \E{}{X Y}$ is the \textbf{(cross)-correlation matrix}.
    \end{definition}

    As for the second term, since $X$ and $\mathcal{E}$ are not independent, we apply the tower property:
    \[
        \E{}{X \mathcal{E}} = \E{X}{\E{}{X \mathcal{E} | X}} = \E{X}{X \E{}{\mathcal{E} | X}} = 0
    \]
    since in the inner expectation $X$ is a constant, it can be taken out of the expectation and since we assumed that the error term is zero mean given $X$, we find out that the whole expectation is zero.

    We can rewrite the equation \ref*{eq:1} as: % TODO: reference to unknown equation (on pdf is not visible the number)
    \[
        R_{XY} =  R_X \beta
    \]
    And then solve for $\beta$:\footnote{By assuming that $R_x$ is invertible.}
    \[
        \beta = R_X^{-1} R_{XY}
    \]
    Finding a new expression for the optimal regression function:
    \[
        r(X) = X^T\beta = X^TR_X^{-1}R_{XY}
    \]
    In conclusion we found out that in a model-based linear regression problem $\beta$ cannot be arbitrary, because it is uniquely determined by the correlation matrices of $X$ and $XY$.
\end{exercise}

\subsection{Connection between model-based and supervised}
\textbf{\textit{What is changing with respect to the other part of the course?}}

In the other part of the course we've worked in a \textbf{supervised} setting, meaning that we had a training set. We still had a model because we \textit{assumed} that the relationship between $Y$ and $X$ was linear, but we wanted to learn the parameters $\beta$ of the model from the data, making it a \textit{parametric regression} problem.

Recall that the prediction phase it's our most important goal and it is the goal we would have if we knew the model. Since we don't have the model, in the \textit{parametric} setting, we need to assume a model and learn the parameters $\beta$. In that case, learning $\beta$ is instrumental to \textit{predict} $Y$.

In our case, we are in a model-based setting, which means we do not have a training set. As we showed in the previous exercise, in our case, the parameters $\beta$ are not to be learned from the data, but they are uniquely determined by the correlation matrices of $X$ and $XY$.

% Sentire registrazione
% If we need information about $\beta$ we are solving inference\dots
% This (regression) is the simplest example of \textbf{MMSE} estimator.
% In practice we assume there is linear relationship and we don't have information about data, e.g. correlation matrix.

%\paragraph*{Clarification}
%When we have an estimation problem, we have our data and our goal is estimate %$Y$ from $X$. If we have one model we have one "theory", e.g. bayesian theory. In estimation, the MMSE is the optimal way to solve the problem.
%
%We have one model but this model is not perfectly known. We try to learn the %model, because our purpose is \textit{still} to estimate $Y$.
%
%In one case we have a \textbf{perfectly known model}, then we can use MMSE %theory, and we can compute the MMSE since we know the model and the distributions. In the other case we have a \textbf{model} but you don't know it perfectly. Then we have to \textbf{learn} the model. Then we need to learn %$\beta$. Note that the goal is still to estimate $Y$.

Now the question is \textbf{\textit{how can we learn the model}}?

In a supervised approach we use a \textbf{training set} $\tilde{X}$ (not a \textit{dataset}).
\[
    \tilde X=
    \begin{bmatrix}
        \tilde X_{11}     & \dots  & \tilde X_{1n}     \\
        \vdots            & \ddots & \vdots            \\
        \tilde X_{(d+1)1} & \dots  & \tilde X_{(d+1)n}
    \end{bmatrix}\\
\]
That we can write compactly as:
\[
    \tilde{X} = \left[
        \tilde{X}_1, \tilde{X}_2, \dots, \tilde{X}_n
        \right]
\]
Where each $\tilde{X}$ is a vector with dimensions $(d+1) \times 1$.

And the corrisponding values of the response variable, $\tilde{Y}$:
\[
    \tilde{Y} =   \left[
        \tilde{Y}_1, \tilde{Y}_2, \dots, \tilde{Y}_n
        \right]
\]
\begin{exercise}
    Propose an estimator $\hat{\beta}$ based on the training set $(\tilde{X}, \tilde{Y})$, knowing that $\beta = R_x^{-1} R_{xy}$ and $Y = \beta^T X$.

    We can find the estimator by replacing the correlation matrices with their empirical counterparts:
    \[
        \hat{\beta} = \left(\hat{R}_X\right)^{-1} \hat{R}_{XY}
    \]
    where the correlation matrices are defined as:
    \[
        [\hat{R}_{X}]_{ij} = \frac{1}{n} \sum_{m=1}^{n} \tilde{X}_{im} \tilde{X}_{mj}
        \qquad
        [\hat{R}_{XY}]_i = \frac{1}{n} \sum_{m=1}^{n} \tilde{X}_{im} \tilde{Y}_{m}
    \]
    % note that component $i$ is the row while component $j$ is the column.
    We can rewrite the estimators by using the matrix notation:
    \[
        [\hat{R}_{X}]_{ij} = \frac{1}{n} \left[\tilde{X}\tilde{X}^T\right]_{ij}
        \qquad
        [\hat{R}_{XY}]_i = \frac{1}{n} \left[\tilde{X} \tilde{Y}^T\right]_i
    \]
    By replacing the correlation matrices in the estimator and simplyfing $n$ we get:
    \[
        \hat{\beta} = \left(\hat{R}_X\right)^{-1} \hat{R}_{XY} = \left(\tilde{X} \tilde{X}^T\right)^{-1}  \tilde{X} \tilde{Y}^T
    \]

    The \textbf{empirical estimator} of $\beta$ we found is the same as the one we obtained in the supervised approach. In that case, we minimized the RSS, which, if divided by $n$, is the empirical risk.
    \[
        RSS = \frac{1}{n} \sum_{m=1}^{n} \left(\tilde{Y}_m - \beta^T \tilde{X}_m \right)^2 \xrightarrow{n \to +\infty} \E{}{(Y - \beta^T X)^2}
    \]
\end{exercise}
%TODO: non ho capito miriam cosa voleva dire qui, riporto le note nei miei appunti e poi ne discutiamo
% \begin{note}{Warning}
%     Sentire la registrazione da qui in poi.
% \end{note}

% The two things are connected only in this case.

% We took the optimal risk (that is the expectation) and we replaced it with the empirical risk. I am trying to do Bayes empirically. The difference is that we need a model. In MMSE we have a \textbf{general solution} that works for every model, in the \textbf{supervised approach} we need to impose or assume a model.

% Optimal Bayes is a benchmark performance. There is some optimal thing to do, we cannot do it. I impose one model and replace the \textit{optimal risk} with the \textit{empirical risk}.
Let's analyze what happened. We fall in a supervised learning problem, where we imposed a shape for the model and we want to learn $\beta$ in order to predict $Y$. There are roughly two possible ways:
\begin{enumerate}
    \item Try to emulate the shape of the optimal $\beta$ by estimating the matrices (correlation and cross-correlation) and then compute the estimator $\hat{\beta}$.
    \item Try to find the best $\beta$ that minimizes the empirical risk. Simply, we try to apply Bayes theory empirically.
\end{enumerate}
In this case, both approaches lead to the same result and are connected from the fact that both are linked to Bayes theory.

\subsection{Benchmark performance}

Both in the model-based and supervised case, the final goal is to make a prediction.

In model-based regression we want to find the expression of $Y$ which allows us to compute $Y$ knowing the distribution of $X$ and the parameters. In supervised regression, we want to make a prediction by exploiting the information contained inside the training set pairs, which must reveal the link between $X$ and $Y$.

Assume we have a training set $T_n = \{(X_i, Y_i)\}_{i=1}^n$, i.e. a set of examples of the link between the data $X$ and the response $Y$, and we want to make a prediction for a new observation $(X_0, Y_0)$. Making a prediction means that we want to predict $Y_0$, that is not observed, given the observation $X_0$. It is important to remark that we are working under the assumption that $X_0$ and $Y_0$ share the same distribution of $X_i$ and $Y_i$ but are outside $T_n$ in order to make it independent by $T_n$!.

In the model-based approach, the optimal estimator is the regression function $r(X_0)$, which is given by the MMSE. In the supervised approach, the estimator is the regression function $\hat{r}(X_0)$, which is given by the minimization of the empirical risk. This function will be a \textbf{suboptimal} estimator, meaning that it will not reach the performance of the MMSE.

\begin{theorem}\label{thm:1}
    The error of the suboptimal regression function $\hat{r}(X_0)$ is given by:
    \[
        \E{}{(\hat{r}(X_0) - Y_0)^2} = \text{MMSE} + \E{}{\left(\hat{r}(X_0) - r(X_0)\right)}
    \]
    where $\text{MMSE} = \E{}{(r(X_0) - Y_0)^2}$ is the error of the optimal estimator.
\end{theorem}

To prove this result, we first need to introduce the \textbf{orthogonality principle}.
\begin{theorem}
    The \textbf{orthogonality principle}, also called \textbf{Fundamental Result of MMSE}, states that the error of the optimal estimator is orthogonal to any (integrable) function of the data $g(X_0)$:
    \[
        \E{}{g(X_0) \left[ r(X_0) - Y_0\right]} = 0\quad \forall g(X_0)
    \]
\end{theorem}

\begin{proof}
    To prove the theorem \ref*{thm:1}, we need to compare the error of the suboptimal estimator with the error of the optimal estimator. The error of $\hat{r}(X_0)$ is given by:
    \[
        \E{}{(\hat{r}(X_0) - Y_0)^2}
    \]
    In order to compare it with the optimal estimator, we need to introduce the term of comparison inside the equation, by adding and subtracting $r(X_0)$ from the error term:
    \[
        \E{}{(\hat{r}(X_0) - Y_0)^2} =  \E{}{(\hat{r}(X_0) - r(X_0) + r(X_0) - Y_0)^2}
    \]
    Let's managing it as a square of binomial:
    \[
        \E{}{(\hat{r}(X_0) - Y_0)^2} =  \E{}{([\hat{r}(X_0) - r(X_0)] + [r(X_0) - Y_0])^2}
    \]
    By expanding the square we get:
    \[
        \E{}{([\hat{r}(X_0) - r(X_0)]^2 + [r(X_0) - Y_0]^2 + 2 [\hat{r}(X_0) - r(X_0)][r(X_0) - Y_0])^2} %TODO: Miriam aveva messo -2, idkw
    \]
    And applying the linearity property:
    \[
        \E{}{[\hat{r}(X_0) - r(X_0)]^2 }+ \E{}{[r(X_0) - Y_0]^2} + 2 \E{}{[\hat{r}(X_0) - r(X_0)][r(X_0) - Y_0]}
    \]
    The first term quantifies how much our regression function is deviated from the optimal regression function, the second term is the MMSE, the third term need further discussion.

    Since in the expectation there are two random variables $X_0$ and $Y_0$ and they are dependent, we cannot split the expectation. In order to get $X_0$ fixed, we would need to have a conditional expectation.

    We apply the \textit{tower property} on the third term:

    \begin{align*}
        \E{}{[\hat{r}(X_0) - r(X_0)][r(X_0) - Y_0]} = \E{X_0} {\E{Y_0}{\underbrace{[\hat{r}(X_0) - r(X_0)]}_{g(X_0)}[r(X_0)-Y_0]  \mid X_0}} = \\
        = \E{X_0} {g(X_0)\E{Y_0}{r(X_0)-Y_0 \mid X_0}} = \\
        = \E{X_0} {g(X_0)\{\E{Y_0}{r(X_0)\mid X_0}-\underbrace{\E{Y_0}{Y_0 \mid X_0}}_{r(X_0)}\}} = \\
        = \E{X_0}{ \E{Y_0}{g(X_0) \left(r(X_0) - r(X_0)\right)}} = 0                                         \\
    \end{align*}

    The last equality is true because of the \textbf{orthogonality principle}.

    So we can rewrite the error of the $\hat{r}(X_0)$ as:
    \[
        \boxed{
            \E{}{(\hat{r}(X_0) - Y_0)^2} = \text{MMSE} + \E{}{\left(\hat{r}(X_0) - r(X_0)\right)}}
    \]
\end{proof}

In conclusion, we can say that the error of the suboptimal estimator is the sum of the error of the optimal estimator and a \textbf{penalty term}, which is the squared difference between the arbitrary function and the optimal estimator. If our arbitrary function is the optimal estimator, the penalty term is zero, but if it differs, the penalty term increases.

This result has been very useful to discover \textit{non-parametric approaches}. Before this result the non-parametric approaches tried to approximate the optimal regression function, but now we can try to find an arbitrary function that minimizes the penalty term. The only problem is that we don't know the optimal regression function, so we cannot compute easily the penalty term. As for the MMSE, it can be approximated by simulation.

\section{Supervised Parametric Regression}
\subsection{Simple Linear Regression}
In order to introduce the concept of \textit{supervised parametric regression}, we will start with the simplest case, the \textit{simple linear regression} on which we assume a linear relationship between the response variable and the predictor variable with the following model:
% Let's consider a simple linear regression model:
\[
    Y = \beta_0 + \beta_1 X + \varepsilon
\]
Where we make the \textbf{zero-mean error assumption}, so $\E{}{\varepsilon} = 0$. 

Assume we want to estimate the parameters $\beta_0$ and $\beta_1$ and find the estimated model:
\[
    \hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X
\]

Suppose we have a training set $\mathcal{D}_{tr} = \{(x_i, y_i)\}_{i=1}^n$. We can write:
\[
    \hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i \qquad i = 1,\dots, n
\]

\paragraph*{Ordinary Least Squares}
The Ordinary Least Squares method let us estimate the parameters $\hat{\beta}_0$, $\hat{\beta}_1$ by minimizing the residual sum of squares, which is defined as following:
\[
    \text{RSS}(\beta_0, \beta_1) = \sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2
\]
Where $e_i$ represents the difference between the observed value and the predicted value, also called \textit{residual}.

Under the assumption that the error term $\underline{\varepsilon} = (\varepsilon_1, \dots, \varepsilon_n)^T$:
\begin{itemize}
    \item $\E{}{\varepsilon_i} = 0, \forall i$
    \item $\var{\underline{\varepsilon}} = \sigma^2 I_n$ (\textbf{homoscedasticity})
\end{itemize}

The \textbf{least square estimates} of the parameters are:
\[
    \underline{\beta}^T = \left(\hat{\beta}_0, \hat{\beta}_1\right) = \arg\min_{(\beta_0, \beta_1)} \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2
\]

If the error terms are independent and identically distributed as normals $\varepsilon_i \sim N(0, \sigma^2)$, then the \textit{least squares estimates} are equal to the ones we obtain by \textit{maximum likelihood estimation}.

\paragraph*{Estimating parameters in simple linear regression}

Now we want to find the least squares estimates for the parameters $\hat{\beta}_0$ and $\hat{\beta}_1$. In order to solve the minimization problem, we need to compute the gradient with respect to the parameters $\underline{\beta}$ and set it to zero:
\[
    \nabla_{\underline{\beta}} RSS(\beta_0, \beta_1) = 0
\]

\[
    \begin{cases}
        \frac{\partial RSS}{\partial \beta_0} = -2 \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i) = 0 \\
        \frac{\partial RSS}{\partial \beta_1} =   -2 \sum_{i=1}^{n} x_i (y_i - \beta_0 - \beta_1 x_i) = 0
    \end{cases}
\]

%TODO: Fare il controllo su punto di minimo con derivata seconda
\[
    \begin{cases}
        n\beta_0 + \beta_1 \sum_{i=1}^n x_i = \sum_{i=1}^{n} y_i \\
        \beta_0 \sum_{i=1}^n x_i + \beta_1 \sum_{i=1}^n x_i^2 = \sum_{i=1}^{n} x_i y_i
    \end{cases}
\]
Let's define the following quantities:
\[
    \overline{x} = \frac{1}{n} \sum_{i=1}^n x_i
    \qquad
    \overline{y} = \frac{1}{n} \sum_{i=1}^n y_i
\]
From the first equation we can find $\beta_0$:
\[
    \beta_0 = \overline{y} - \beta_1 \overline{x}
\]
Substituting this value into the second equation we can find $\beta_1$:
\[
    \overline{y} \sum_{i=1}^n x_i - \beta_1 \overline{x} \sum_{i=1}^n x_i + \beta_1 \sum_{i=1}^n x_i^2 = \sum_{i=1}^{n} x_i y_i \leftrightarrow \beta_1 = \frac{\sum_{i=1}^{n} x_i y_i - \overline{y} \sum_{i=1}^n x_i}{\sum_{i=1}^n x_i^2 - \overline{x} \sum_{i=1}^n x_i}
\]

The least squares estimates are the following:
\[
    \begin{cases}
        \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} \\
        \hat{\beta}_1 = \frac{\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}{\sum_{i=1}^n (x_i - \overline{x})^2}
    \end{cases}
\]

\callout{Note}{Some books indicates the previous with $B_0$ and $B_1$ because they have the meaning of \textit{estimators}. For notation simplicity, we will use $\hat{\beta}_0$ and $\hat{\beta}_1$.}
The \textbf{Gauss-Markov} theorem states that, in a model where the errors are zero-mean, incorrelated and homoschedastic, the least squares estimates are the \textbf{best linear unbiased estimates} (BLUE) of the parameters $\beta_0$ and $\beta_1$.

%NOTA: non mi trovo con quello che ha scritto Miriam, quello indicato è l'RSE ed è la stima della dev std di epsilon non di beta. INoltre i beta non hanno la stessa varianza
% This means that the least squares estimates are unbiased and have the \textit{same} smallest variance:
% \[
%     \sigma^2 = \text{RSE}^2 = \frac{\text{RSS}(\hat{\beta}_0, \hat{\beta}_1)}{n-2}
% \]
\[
    \E{}{\hat{\beta}_0} = \beta_0
    \qquad
    \E{}{\hat{\beta}_1} = \beta_1
\]

The \textbf{variance} of the estimates made using OLS is given by:
\[
    \text{SE}(\hat{\beta}_0)^2 = \sigma^2\left[\frac 1n + \frac{\bar x^2}{\sum_{i=1}^n(x_i-\bar x)^2}\right] 
    \qquad
    \text{SE}(\hat{\beta}_1)^2 = \frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar x)^2}
\]

where $\sigma^2$ is the variance of the error term $\varepsilon$ and it is unknown. We can estimate it using the \textbf{residual standard error} under the homoschedasticity assumption:
\[
    \text{RSE} = \sqrt{\frac{1}{n-2} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2} = \sqrt{\frac{1}{n-2} \text{RSS}}
\]

At the denominator of the RSE there is, in general case, $n-p-1$ where $p$ is the number of predictors. In the simple linear regression case, $p=1$ and the denominator is $n-2$. It is also known as the \textbf{degrees of freedom} of the model.

\subparagraph*{Confidence intervals}
The \textbf{confidence intervals} tells us the interval on which, after repeating an estimation experiment many times, we would expect the true value of the parameter to fall given a certaing \textbf{confidence level} or \textbf{error level}.
\callout{Note}{The \textit{error level} is mandatory since, otherwise, the intervall should be from $-\infty$ to $+\infty$.}
The assumptions to make about the errors $\varepsilon_i$ are the following:
\begin{itemize}
    \item \textbf{Homoschedasticity}: the errors $\varepsilon_i$ have the same variance
    \item \textbf{Independence}: the errors $\varepsilon_i$ are independent
    \item \textbf{Normality}: the errors $\varepsilon_i$ are normally distributed.\footnote{This assumption is not necessary to compute the least squares estimates, but it is necessary to compute the confidence intervals.}
\end{itemize}
% By assuming that the errors $\varepsilon_i$ are normally distributed, we can compute the confidence intervals for the parameters $\beta_0$ and $\beta_1$:
It's useful to compute that intervals for $\beta_0$ and $\beta_1$. 
Let's call $B_0$ and $B_1$ the random variables that represent the estimates of $\beta_0$ and $\beta_1$ respectively. Since we know the distribution of the errors $\varepsilon_i$, we can compute the distribution of $B_0$ and $B_1$:
\[
    B_0 \sim N(\beta_0, SE^2(B_0))
\]
\[
    B_1 \sim N(\beta_1, SE^2(B_1))
\]

The following quantity:
\[
    \frac{B_1 - \beta_1}{SE(B_1)} \sim t_{n-2}
\]
is called \textbf{t-statistic} and it is used to compute the confidence interval for $\beta_1$. Since it is the ratio between a standard normal distribution and the square root of the ratio between a chi-square distribution and a number of degree of freedom, It is distributed as a (Student) $t$-distribution with $n-2$ degrees of freedom. %TODO: si potrebbe spiegare molto meglio. Se vogliamo allungare possimao farlo
%TODO: si potrebbe aggiungere immagine

By fixing an error level $\alpha$, we can define the inverval by knowing that:
\[
    \Pr{-t_{1-\alpha/2, n-2} \leq \frac{B- \beta}{SE(B)} \leq t_{1-\alpha/2, n-2}} = 1 - \alpha
\]
where we have not specificied the subscript for generality.

Since we want to know the interval where we can find $\beta$, we can write:
\[
    \Pr[B+SE(B)\cdot t_{1-\alpha/2, n-2} \leq \beta \leq B+SE(B)\cdot t_{1-\alpha/2, n-2}] = 1 - \alpha
\]
Evalutaing the estimator $B$ we obtain $\hat \beta$ and we can compute the interval:
\[
    L = \hat \beta - SE(\hat \beta) \cdot t_{1-\alpha/2, n-2}
    \qquad
    U = \hat \beta + SE(\hat \beta) \cdot t_{1-\alpha/2, n-2}
\]
The true value of $\beta$ is expected to fall in the interval $[L, U]$ with a confidence level of $1-\alpha$.

\subsection{Hypothesis testing}
In the simple linear regression case, we would like to know if the predictor $X$ has a significant effect on the response $Y$. In order to do that, we can perform a \textbf{hypothesis test} on the parameter $\beta_1$.
If that coefficient is not significantly different from zero, then the predictor $X$ has no effect on the response $Y$.

The hypotesis test is made by the \textit{null hypothesis} $H_0$ and the \textit{alternative hypothesis} $H_1$:
\[\begin{cases}
    H_0: \beta_1 = \beta^\ast_1 \text{ There is no relationship}\\
    H_1: \beta_1 \neq \beta^\ast_1 \text{ There is a relationship}
\end{cases}\]
This kind of test is said to be \textbf{bilateral} since, if the null hypothesis is rejected, we can say that there is a relationship between $X$ and $Y$, but we cannot say if it is positive or negative.
As we've done for the confidence intervals, we have to accept an error level $\alpha$.

In order to perform the test we need a \textbf{test statistic}.
Let's suppose that we want to test if $\beta_1$ is equal to a certain value $\beta_1^\ast$ (in our case $\beta_1^\ast = 0$). We can define the following test statistic:
\[
    t = \frac{B_1 - \beta_1^\ast}{SE(B_1)}
\]
We can note that, if the null hypothesis is true, the test statistic is distributed as a $t$-distribution with $n-2$ degrees of freedom. 
We can use this information to decide if we can reject the null hypothesis or not. If the evaluated test statistic is far from zero, probably the true value of $\beta_1$ is different from zero and we can reject the null hypothesis. If the test statistic is close to zero, we cannot reject the null hypothesis.

To be able to choose between the hypothesis we need to impose a threshold. For this reason we introduced $\alpha$, also called \textbf{I type risk}:
\[
    \Pr{\text{Reject } H_0 \mid H_0 \text{ is true}} = \alpha
\]
%TODO: inserire un'immagine
$\alpha$ is the area under the tails of the $t$-distribution when the null hypothesis is true, this area is called \textbf{critical region}.

Since the decision we make can have a very different meaning if the distance from 0 is greater or smaller, we introduce a value that measure the strongness of the evidence against the null hypothesis. This value is called \textbf{p-value} and it is the probability of observing a value of the test statistic greater than the one observed, assuming that the null hypothesis is true. The more the p-value is small, the more the evidence against the null hypothesis is strong. Formally:
\[
    p = \Pr{|t| \geq |t'|    \mid H_0 \text{ is true}}
\]
We can sintetize the decision rule as following:
\begin{itemize}
    \item If $p \leq \alpha$ we reject the null hypothesis
    \item If $p > \alpha$ we cannot reject the null hypothesis
\end{itemize}
%─────────────────────── THIS SLIDE ──────────────────
\subsection{Assessing the accuracy of the model}
In order to understand if the inffered model is good enough, we should use an error metric. If we're working with a continuous target variable, we can use the \textit{RSE} or \textbf{residual standard error}. The residual standard error measures how much of the variability is not explained by our model.

However, the residual standard error has the limitation that it depends on scale by the order of $Y$. If $Y$ is a large number, then the \textit{RSE} will also be a large number.

This means that it is not always clear what constitutes a good RSE. An alternative measure of fit is the $\mathbf{R^2}$ \textbf{statistic}.

This statistic represents the proportion of variance explained and it is independent of the scale of $Y$. The definition of \textit{R-squared} is the following:
\[
    R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}
\]
Where \textbf{TSS} is the \textit{total sum of squares} and it is defined as following:
\[
    TSS = \sum_{i=1}^{n} (y_i - \overline{y})^2
\]
This quantity measures the total variance in the response $Y$ and represents the amount of variability inherent in the response before the regression is performed. In contrast, RSS measures the amount of variability that is left unexplained after performing the regression.

The difference between this two quantities measures the amount of variability in the response that is explained or removed by performing the regression and $R^2$ measures the proportion of variability in $Y$ that can be explained using $X$.
When the $R^2$ statistic is close to $1$ it means that a large proportion of the variability in the response has been explained by the regression.
When the $R^2$ statistic is close to $0$ it means that the regression did not explain much of the variability in the response and this might occur because the linear model is wrong or the inherent error is high or both.

In the simple regression setting:
\[
    R^2 = r^2
\]
Where $r$ is the Pearson's correlation index between $X$ and $Y$:
\[
    r = \frac{\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \overline{x})^2} \sqrt{\sum_{i=1}^{n} (y_i - \overline{y})^2}}
\]
The correlation index is a measure of the linear relationship between $X$ and $Y$. The $R^2$ statistic extends the concept of correlation between multiple predictors and the response.

\subsection{Multiple Linear Regression}
Suppose we have an input vector $X^T = (X_1, \dots, X_p)$ and we want to predict an output $Y$. The linear regression model that we can assume in this setting has the shape:
\[
    Y = f(X) + \varepsilon = \beta_0 + \sum_{j=1}^{p} \beta_j X_j + \varepsilon
\]

We can interpret the model as the following: $\beta_j$ is the average increase in $Y$ when $X_j$ increases by one unit holding all others constants.

Note that despite being called \textbf{linear regression}, $X_j$ can come from different sources, such as:
\begin{itemize}
    \item transformations of the input function, e.g. $\log$
    \item polynomial fit, e.g. $X_2 = X_1^2$
    \item dummy coding of the levels of qualitative inputs
    \item interaction between variables $X_3 = X_1 \cdot X_2$
\end{itemize}


\subsubsection*{Least squares estimates}
As we said, given the input vector $X^T = (X_1, \dots, X_p)$, if we want to predict an output $Y$, we need a regression model, for example:
\[
    Y = f(X) + \varepsilon
\]
That model, under a linear assumption, become:
\[
    Y = \beta_0 + \sum_{j=1}^{p} \beta_j X_j + \varepsilon
\]
Assuming a zero-mean error, we can make prediction using the following model:
\[
    \hat{Y} = \hat{\beta}_0 + \sum_{j=1}^{p} \hat{\beta}_j X_j
\]
To apply the OLS approach, it's convenient to reason in a matrix form. 
\begin{gather*}
    \underline{X} = (X_1, \dots, X_p)\text{ input vector} \\
    \underline{\hat\beta} = (\hat\beta_0, \hat\beta_1, \dots, \hat\beta_p)^T\text{ parameter estimates vector} \\
\end{gather*}

Given the training data $\mathcal{D}_{tr} = \{ (x_i, y_i) \}_{i=1}^n$
we pick $\underline{\hat\beta}$ as the vector that minimize the residual sum of squares.
\[
    \text{RSS}(\underline \beta) = \sum_{i=1}^{n} (y_i - f(x_i))^2 = \sum_{i=1}^{n} \left(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}\right)^2
\]
We can write our linear regression model in matrix form:
\[
    \underline Y = 
    \overset{[n\times 1]}{
        \begin{pmatrix}
            y_1 \\ \vdots \\ y_n 
        \end{pmatrix}
        } =
    \overset{[n\times(p+1)]}{
        \begin{pmatrix}
            1 & x_{11} & x_{12} & \dots & x_{1p} \\
            1 & x_{21} & x_{22} & \dots & x_{2p} \\ \vdots \\ 1 & x_{n1} & x_{n2} & \dots & x_{np}
        \end{pmatrix}
        }
    \overset{[(p+1)\times 1]}{
            \begin{pmatrix}
                \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p 
            \end{pmatrix}
            } +
    \overset{[n\times 1]}{
        \begin{pmatrix}
            \epsilon_1\\
            \vdots \\
            \epsilon_n
        \end{pmatrix}}= \underline{\underline{X}} \underline \beta + \underline \epsilon
\]

So the estimated response on the traning set will be:
\[
    \hat{Y} = \underline{\underline{X}} \underline{\hat{\beta}}
\]
Now we can rewrite also the formula of the \textit{RSS} in matrix form:
\[
    \text{RSS}(\underline{\beta})  = \sum_{i=1}^{n} \left(y_i - (\underline{\underline{X}} \underline{\beta})_i\right)^2 = (\underline{Y} - \underline{\underline{X}} \underline{\beta})^T(\underline{Y} - \underline{\underline{X}} \underline{\beta})
\]
Now we need to compute the gradient of the residual sum of squares with respect to the parameters. Let's recall first the following rules:
\begin{enumerate}
    \item $\nabla_x a^t x = \nabla_x x^T a =  a$
    \item $\nabla_x x^T A x = A^t x + A x$
\end{enumerate}
The gradient of the RSS will be (underline omitted for simplicity):
\begin{align*}
    \nabla_\beta \text{RSS} (\beta) & = \nabla_\beta \left((Y- X\beta)^T (Y-X\beta) \right) =                                 \\
                                    & = \nabla_\beta \left(Y^T Y - \underbrace{Y^T X\beta}_{(1\times 1)} - \underbrace{\beta^T X^T Y}_{(1\times 1)} + \beta^T X^T X \beta \right) = \\
                                    & = \nabla_\beta\left(Y^T Y - 2 \beta^T X^T Y + \beta^T X^T X \beta\right) =              \\
                                    & = -2 X^T Y + \underbrace{(X^T X)^T \beta + (X^T X)^T \beta}_{\text{Second rule of gradient}} 
                                    % =                                        \\
                                    % & = X^T Y + (X^T X)^T \beta = 0 \iff \hat{\beta} = (X^T X)^{-1} X^T Y
\end{align*}
We can set the gradient to zero and solve for $\beta$:
\begin{align*}
    -2 X^T Y + 2(X^T X)^T \beta = 0 \implies \\
    X^T Y - (X^T X)^T \beta = 0 \implies \\
    (X^T X)^T \beta = X^T Y \implies \\
    \beta = (X^T X)^{-1} X^T Y    
\end{align*}


The \textit{least squares estimation} of the parameters are
\[
    \hat{\underline\beta} = (\underline {\underline X}^T \underline{\underline X} )^{-1} \underline{\underline X}^T \underline Y
\]

Note that durning the computation we also found that:
\[
    X^T Y - (X^T X)^T \hat \beta = X^T\underbrace{(Y-X\hat{\beta})}_{\underline {e}} = 0 \Rightarrow e \perp X
\]
Namely, we found that the vector $\hat{\underline \beta}$ that minimizes RSS, is chosen in such a way that the residual vector $\underline e$ is \textbf{orthogonal} to the data matrix $\underline {\underline X}$ that represents the subspace on which our predictors move.

Finally, to confirm that this solution is actually a point of minimum, we need to check the \textbf{hessian matrix}, defined as following:
\[
    \underline {\underline H}_{RSS}^{(\underline\beta)} = \frac{\partial^2 RSS(\underline\beta)}{\partial \underline\beta \partial \underline\beta^T} = 2 \underline {\underline X}^T \underline {\underline X}
\]
It can be shown that $\underline {\underline H}_{RSS}^{(\underline\beta)} = 2 \underline {\underline X}^T \underline {\underline X}$ is an estimator of the \textbf{variance-covariance matrix} of the predictors (columns of $\underline {\underline X}$). By definition, this matrix is \textit{positive semidefinite}\footnote{A matrix $A$ is positive semidefinite if the eigenvalues $\lambda_i$ of the matrix $A$ are greater or equal to 0} and this means that the estimator is always a point of minimum.

Now we can substitute the parameters into $\hat{Y}$ to make predictions:
\[
    \underline{\hat{Y}} = \underline{\underline{X}} \underline{\hat{\beta}} =   \underbrace{\underline{\underline{X}} (\underline {\underline X}^T \underline {\underline X})^{-1} \underline {\underline X}^T}_{\underline {\underline H}} \underline Y = \underline {\underline H}\underline Y
\]
We've found that $\hat{\underline Y}$ depends directly on the original response and the constant of proportionality is a matrix $\underline {\underline H}$ called \textit{hat matrix}.
We can see $\underline {\underline H}$ as a matrix that projects $\underline Y$ on the data $\underline {\underline X}$, sometimes called \textbf{design matrix}, in order to compute $\hat{\underline Y}$.

\paragraph*{A geometrical view of least squares}
Let's consider least squares regression with two predictors. $\hat{\underline Y}$ is nothing but the projection of $\underline Y$ obtained through $\underline {\underline H}$ onto the space (plane, in this case) of the predictors. All that we cannot explain through the predictors is the component of $\underline Y$ orthogonal to this space.
% The outcome vector $y$ is orthogonally projected onto the hyperplane spanned by the input vectors $x_1$ and $x_2$. The projecttion $\hat{y}$ represents the vector of the least squares predictions.

%TODO inserire immagine altrimenti non si capsice
\paragraph*{Variance of least squares estimator}
Assuming that:
\begin{itemize}
    \item the input vector $x_i$ are non-random
    \item the errors $\varepsilon_i$ are iid
    \item homoschedasticity: $\E{}{\varepsilon_i} = 0$
    \item zero-mean error: $\var{\varepsilon_i} = \sigma^2$
\end{itemize}   
The \textbf{variance-covariance matrix} of the least squares estimates can be computed as:
\[
    \var{\hat{\underline\beta}} = \sigma^2 (\underline {\underline X}^T \underline {\underline X})^{-1}
\]
Again, from the \textit{Gauss-Markov} theorem, the least squares estimates are the \textbf{best linear unbiased estimates} (BLUE) of the parameters $\underline \beta$.

If we don't known $\sigma^2$, we can compute an unbiased estimate of the variance parameter with the RSE:
\[
    \hat{\sigma}^2 = \frac{1}{n-p-1} RSS = \frac{1}{n-p-1} \sum_{i=1}^{n} (y_i -\hat{y}_i)^2
\]

\subsection*{Analysis of the regression model}
In order to do some considerations about the model, we will assume:
\begin{itemize}
    \item the linear model is the correct population model
    \item $\varepsilon_i \sim^{iid} N(0,\sigma^2)$
\end{itemize}

Once made that assumption we also get that $\hat{\underline\beta}$ is multivariate normal:
\[
    \hat{\underline\beta} \sim N(\underline\beta, \sigma^2 (\underline {\underline X}^T \underline {\underline X})^{-1})
\]
Another important aspect is that the estimator of the variance $\hat{\sigma}^2$ has its own distribution:  
\[
    (n-p-1) \hat{\sigma}^2 \sim \sigma^2 \chi^2_{n-p-1}
\]
and $\hat{\underline\beta}$ and $\hat{\sigma}^2$ are statistically independent.

% \paragraph*{Hypothesis testing}
% In order to answer the question \textit{is a particular $X_j$ predictor important?}.
% We could answers using the following hypotesis test:
% \[
%     \begin{cases}
%         H_0: \beta_j = 0\\ H_A:\beta_j \neq 0
%     \end{cases}
% \]
% To run this test, we calculate the $t$-statistic:
% \[
%     t_j = \frac{\hat{\beta}_j}{SE(\hat{\beta}_j)} = \frac{\hat{\beta}_j}{\hat{\sigma} \sqrt{v_j}}
% \]
% where $v_j$ is the $j$-th diagonal element of $(X^T X)^{-1}$

% CONTINUARE DA QUIII
\paragraph*{Is there a relationship between the response and predictors?}
By working with the multiple linear regression model, we could ask ourselves if at least one of the selected predictors is associated with the response. In order to answer this question, we can construct an hypothesis test called \textbf{$F$-test}:
\[
    \begin{cases}
        H_0: \beta_1 = \beta_2 = \dots = \beta_p = 0\\
        H_A: \text{At least one } \beta_j \text{ is different from zero}
    \end{cases}
\]
This kind of test can be solved using the \textbf{$F$-statistic}:
\[
    F = \frac{(TSS - RSS)/p}{RSS/(n-p-1)}
\]
If the linear hypothesis are true, it can be shown that:
\[
    \E{}{RSS/(n-p-1)} = \sigma^2
\]
Moreover, if the null hypothesis is true, it can be shown that:
\[
    \E{}{(TSS - RSS)/p} = \sigma^2
\]
This means that if the null hypothesis is true, the $F$-statistic will be close to $1$. If the null hypothesis is false, the $F$-statistic will be greater than $1$.
When the null hypothesis is true, the $F$-statistic follows an \textbf{$F$-distribution}, called \textbf{Fisher-Snedecor distribution} with $p$ and $n-p-1$ degrees of freedom.

It can be useful to test the hypothesis that just a subset of the predictors have a relationship with the response. In order to do that, we can use the \textbf{partial $F$-test}:
\[
    \begin{cases}
        H_0: \beta_{p-q+1} = \dots = \beta_{p} = 0\\
        H_A: \text{At least one is different from zero}
    \end{cases}
\]
The $F$-statistic for the partial test is:
\[
    F = \frac{({RSS}_0 - RSS)/q}{RSS/(n-p-1)}
\]
where ${RSS}_0$ is the residual sum of squares for a model that contains all the predictors except for those in question.

Note that perform a $t$-test for each variable is different from perform a $F$-test for the whole model. The $F$-test is a \textit{joint test} of the significance of a group of variables, while the $t$-test is a \textit{individual test} of the significance of a single variable.
For example, if we perform a test with $p=100$ predictors and we use $\alpha = 0.05$, we would expect to see 5 predictors with $p$-value less than $0.05$ even if the null hypothesis of $F$-test is true. This is called \textit{multiple comparison problem}.
$F$-statistic adapt itself to the number of predictors, so if $H_0$ is true, there is just the $5\%$ of chance to have a p-value less than $\alpha$.

\paragraph*{Variable Selection}
The F-statistic is one of the techniques adopted to select the predictors that are associated with the response. The task of determining the predictors is called \textbf{variable selection}.

The \textit{prediction accuracy} is improved by performing the variable selection, because the least squares estimates have low bias but large variance. The variance is reduced by fitting a model that only contains the predictors that are actually associated with the response.

\textit{Interpretation} is another reason to perform variable selection. We want to identify a smaller subset of predictors with the strongest relationship with the response.

Ideally we could perform variable selection using a \textit{brute force approach} by fitting all the possible subset of the predictors and then choose between them based on some criterion. This is the most computationally expensive method because with $p$ predictors we have $2^p$ possible models cause each predictor has $2$ possible state. This approach can be considered only if $p$ is small.

In order to judge the quality of a model we can use several criterion such as Mallow's $C_p$, Akaike information criterion (AIC), Bayesian information criterion (BIC), adjusted $R^2$.

There are also other \textit{algorithms} to perform variable selection, such as \textit{best subsets selection}, \textit{forward selection}, \textit{backward selection}, \textit{stepwise selection}.

\subparagraph*{Forward selection} which starts with the \textit{null model} (only intercept term). At each step fit $p$ simple linear regressions and add to the model the variable that results in the lowest RSS, then add to that model the variable that results in the lowest RSS for the new two-variable model, and so on, until some stopping rule is satisfied.

\subparagraph*{Backward selection} which starts with all the variables in the model, then removes the variable with the largest p-value, that is the variable that is the least statistically significant. The new $(p-1)$-variable model is fit, and the variable with the largest p-value is removed. This procedure continues until a stopping rule is reached. This technique cannot be used if the number of predictors is greater than the number of samples, while forward selection can always be used.

\subparagraph*{Mixed selection} which starts with no variable in the model and add the variable that provides the best fit. However, if at any point the $p$-value for one of the variables in the model rises about a certain threshold, we remove it from the model. We continue to perform both the forward and backward steps until all variables in the model have a sufficiently low $p$-value and all variables outside the model would have a large p-value if added to the model.

An important thing to note is that while an $R^2$ value close to $1$ indicates that the model explains a large portion of the variance in the response variable, it does not indicate that the model will predict future observations with great accuracy. In fact, $R^2$ will always increase when more variables are added to the model, even if only weakly associated with the response. This is because the $R^2$ statistics is computed on the training data and indicates a better fit on those data.

% When fitting a model, it can be useful to plot the data if it is possible.

\subsection*{Confidence and prediction intervals}
After fitting the multiple regression model, it is possible to predict $Y$ on the basis of a set of values for the predictors $X_1, X_2, \dots, X_p$. However, there are three type of uncertainty associated with this prediction:
\begin{itemize}
    \item the coefficient estimates $\hat{\underline\beta}$ are only estimates of the true values of the coefficients $\underline\beta$.
    \item we assumed a linear model for $f(X)$, but the true relationship between the predictors and the response may not be linear, this is called \textbf{model bias}.
    \item even if the know exactly $f(X)$, the error term $\varepsilon$ is still present and it is a source of uncertainty. We called this \textbf{irreducible error}.
\end{itemize}

We can use the \textbf{confidence intervals} to quantify the uncertainty surrounding the estimates of the coefficients $\underline\beta$. 
But we can also use the \textbf{prediction intervals} to quantify the uncertainty surrounding the prediction of $Y$ for a given value of $X$.

\subsubsection*{Confidence intervals}
As we said for simple linear regression, we have to assume a normal distribution for the error term $\varepsilon$:
\[
    \varepsilon_i \sim N(0, \sigma^2)
\]
Given a new input vector $X_0 = (x_{01}, x_{02}, \dots, x_{0p})$, we want to create a confidence interval for $f(X_0)=\E{}{Y|X = X_0} = X_{new}^T\cdot \underline \beta$, where $X_{new} = (1, X_0)$.
We can note that:
\[
    X_{new}^T\cdot \hat{\underline \beta} \sim \mathcal N (X_{new}^T\cdot \underline \beta, \sigma^2 X_{new}^T(\underline {\underline X}^T \underline {\underline X})^{-1} X_{new})
\]

The confidence interval for $f(X_0)$ with a confidence level of $1-\alpha$ is:
\[
    \hat{f}(X_0) \pm t_{1-\alpha/2, n-p-1} \cdot \hat{\sigma} \sqrt{X_{new}^T(\underline {\underline X}^T \underline {\underline X})^{-1} X_{new}}
\]
We could also use the quantile of the standard normal distribution if we assume that the number of samples is large enough ($n > 30$).

\subsubsection*{Prediction intervals}
Again, we have to assume:
\[
    \varepsilon_i \sim \mathcal N (0,\hat\sigma^2)
\]
and the Independence by $X$.

For a new input vector $X_0 = (x_{01}, x_{02}, \dots, x_{0p})$, we want to create a prediction interval for $Y_0 = f(X_0) + \varepsilon = X_{new}^T\cdot \underline \beta + \varepsilon$, where $X_{new} = (1, X_0)$.
We can note that:
\[
    X_{new}^T\hat{\underline \beta} + \varepsilon \sim \mathcal N (X_{new}^T\cdot \underline \beta, \sigma^2 + \sigma^2 X_{new}^T(\underline {\underline X}^T \underline {\underline X})^{-1} X_{new})
\]

The prediction interval for $Y_0$ with a confidence level of $1-\alpha$ is:
\[
    \hat{f}(X_0) + \varepsilon \pm t_{1-\alpha/2, n-p-1} \cdot \hat{\sigma} \sqrt{1 + X_{new}^T(\underline {\underline X}^T \underline {\underline X})^{-1} X_{new}}
\]
\subsection*{Qualitative predictors}
The predictors inside a multiple linear regression model can be both quantitative and qualitative. The qualitative predictors are also called \textbf{categorical} or \textbf{factor} variables. 
The \textbf{levels} of a qualitative predictor are the different values that the predictor can assume. We can use \textbf{dummy variables} to represent the levels of a qualitative predictor. 

A dummy variable is a binary variable that indicate the presence or absence of some categorical effect that may be expected to shift the outcome. For each categorical predictor we need to create $k-1$ dummy variables, where $k$ is the number of levels of the predictor. This process of creating that variables is also called \textbf{one-hot encoding}.
Each dummy variable adopt a \textbf{coding scheme} that manage the values assumed by the dummy variables. 

For example, suppose to have e qualitative predictor with three levels: South, West, East. We can create two dummy variables with a 0/1 coding scheme as following:
\[
    \begin{cases}
        X_1 =  
        \begin{cases}
            1 & \text{if the observation is in the South} \\
            0 & \text{otherwise}
        \end{cases}\\
        X_2 = 
        \begin{cases}
            1 & \text{if the observation is in the West} \\
            0 & \text{otherwise}
        \end{cases}
    \end{cases}
\]
The model will have the following shape:
\[
    Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \varepsilon  = 
    \begin{cases}
        \beta_0 + \varepsilon & \text{if the observation is in the East} \\
        (\beta_0 + \beta_1) + \varepsilon & \text{if the observation is in the South} \\
        (\beta_0 + \beta_2) + \varepsilon & \text{if the observation is in the West}
    \end{cases}
\]
Observe that, if we used a different coding scheme as -1/1, the model would have a different shape.
\subsection*{Extensions of the linear model}
With the model we've seen so far, we're able to reach good results in many cases. However, we've done 2 important assumptions that could be limiting in some cases:
\begin{enumerate}
    \item \textbf{Linearity}: we've assumed that with a unitary change of the predictors, the response changes constantly. 
    \item \textbf{Additivity}: we've assumed that the effect of a change in a predictor is independent from the other predictors.
\end{enumerate}
We try to overcome these limitations.
\subsubsection*{Interactions}
A simple way to estend the linear model is to include \textbf{interaction terms}. An interaction term is the product of two (or more) predictors.
For example, suppose we have a model with two predictors $X_1$ and $X_2$, the interaction term will be $X_1 \cdot X_2$. The model will have the following general shape:
\[
    Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 \cdot X_2 + \varepsilon
\]
Introducing this terms allows to relax the additivity assumption. In this example, the effect of a change in $X_1$ on $Y$ depends on the value of $X_2$ and vice versa and it can be shown:
\[
    Y = \beta_0 + (\beta_1+\beta_3X_2)X_1 + \beta_2X_2 + \varepsilon 
\]
\subsubsection*{Non-linear effects of predictors}
To overcome the limitation of the linearity assumption, we can use \textbf{polynomial regression}. This extension of multiple linear regression allows to introduct regressors that are non-linear functions of the predictors. 
A general shape of a polynomial regression model is:
\[
    Y = \beta_0 + \sum_{k=1}^p \beta_kX^k + \varepsilon
\]

\subsection*{Potential problems}
When we fit a linear regression model to a particular data set, many problems may arise. Most of them are related to the assumption that we've previously done. The most common of these are:

\begin{enumerate}
    \item Non-linearity of the response-predictors relationship
    \item Correlation of error terms
    \item Non-constant variance of error terms
    \item Outliers
    \item High-leverage points
    \item Collinearity
\end{enumerate}

\subsubsection*{Non-linearity of the response-predictors relationship}
The linear regression model assumes that there is a \textit{straight-line relationship} between the predictors and the response. If the true relationship is far from the linear axis, then virtually all conclusions drawn from the fit are suspect. In addition, the model's predictive accuracy would be significantly reduced.

The \textbf{residual plots} is a useful graphical method for identifying non-linearities.
Given a simple linear regression model we can plot the residuals $e_i=y_i-\hat y_i$ towards the predictor $x_i$.
In the case of a multiple regression model, since we have multiple predictors, we can plot the residual towards the predicted (or fitted) value $\hat y_i$.

Ideally, the residual plot should show no obvious patterns. 
The presence of patterns such as a \textit{U-shape} or an \textit{inverted U-shape} may indicate that the relationship between the predictors and the response is non-linear.
An example is shown in figure \ref{fig:residuals_example}.
\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{./figures/chapter_3/residuals_example.png}
    \caption{On the left a residual plot that show a pattern. On the left the same but without visible patterns.}
    \label{fig:residuals_example}
\end{figure}

In that cases, we can use \textbf{non-linear transformation} of the predictors or the response to overcome the non-linearity. For example, we can use $log X$, $X^2$, $\sqrt{X}$, $1/X$ and so on.
\subsubsection*{Correlation of error terms}
An important assumption of the linear regression model is that the error terms $\varepsilon_i$ are uncorrelated.

This means, for example, that the fact that $\epsilon_i$ is positive provides no information (or minimally) about the sign of $\epsilon_{i+1}$.

The standard errors of the estimated coefficients or the fitted values themselves are based on the assumption of uncorrelation of the error terms.

If, in fact, there was correlation between these error terms, then the estimates of the standard errors will greatly underestimate their true value. And as a result, the confidence and prediction intervals will be narrower than they should be. For example:

- A confidence interval at $95\%$ may actually have a much lower probability than $0.95$ of containing the true value of the parameter
- $p$-values associated with the model will be more basic than they should be; leading to the erroneous conclusion that some parameters are statistically significant

In essence, we would be led to have an unjustified sense of confidence in our model.

These correletions could arise from \textbf{time series}, i.e. observations that are collected over time, where the error terms may be correlated across time. 

By the way, the correltions could arise also for other reasons. For example, if we're trying to predict the height of people basing on their weight, we could be wrong if we assume error term incorrelation because some people can belong to the same family or can have the same lifestyle.
\subsubsection*{Non-constant variance of error terms}
Confidence intervals, prediction intervals, hypotesis tests, standard errors and $p$-values are all based on the assumptio of \textbf{homoschedasticity}.
Unfortunately, this assumption is often violated in practice. The presence of \textbf{heteroschedasticity} can be identified by the presence of a \textit{funnel shape} in the residual plot, as shown in the left panel of figure \ref{fig:eteroschedasticity_example}.
\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{./figures/chapter_3/eteroschedasticity_example.png}
    \caption{On the left a residual plot that show a funnel shape. On the left the same but without visible patterns.}
    \label{fig:eteroschedasticity_example}
\end{figure}

A possible solution to this problem is to transform the response $Y$ using a concave function such as $log Y$ or $\sqrt{Y}$. This transformation involves a greater contraction of the broader responses, therefore leading to a reduction in heteroschedasticity as shown in right panel of figure \ref{fig:eteroschedasticity_example}.
\subsubsection*{Outliers}
An \textbf{outlier} is a point for which the response $Y_i$ is far from the value predicted by the model. Outliers can have a large effect on the estimated coefficients and can have a significant impact on the fit of the model.
They can emerge from a variety of reasons, such as:
\begin{itemize}
    \item Data entry errors
    \item Measurement errors
    \item Sampling errors
    \item Unusual observations
\end{itemize}
The presence of outliers can be identified by the presence of points that are far from the others in the residual plot. By the way, it can be difficult to identify a threshold that separates the outliers from the other points. For that reason, a common approach is to use the \textbf{studentized residuals} $\tau_i$, defined as a normalization of classical residuals:
\[
    \tau_i = \frac{e_i}{\hat{\sigma}\sqrt{1-h_{ii}}}
\]
where $h_{ii}$ is the $i$-th diagonal element of the hat matrix $\underline {\underline H}$. An empirical rule is that if $|\tau_i| > 3$, then the $i$-th observation is an outlier.

If we think that the outlier is due to a mistake, we can remove it from the dataset. But if we think that the outlier is due to a deficiency of the model, we must preserve it and try to enhance the model.
\subsubsection*{High-leverage points}
A \textbf{high-leverage point} is an observation that has a value of the predictor that is far from the other values of the predictor. As the outliers, these points can have a large effect on the estimated coefficients and can have a significant impact on the fit of the model.
In the case of a simple linear regression model, a high-leverage point is an observation with an extreme value of the predictor $X$ and it can be simply identified by the presence of points that are far from the others in the scatter plot of the response $Y$ against the predictor $X$.
In the case of multiple linear regression model, the identification of high-leverage points is more complex. A point can be in the range for each predictor individually, but can be inusual in terms of all the predictors. 

A common approach is to use the \textbf{leverage statistic} $h_{i}$. For the simple regression is defined as:
\[
    h_{i} = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\qquad h_i\in\left[\frac 1n, 1\right]
\]
There is also a generalization for the multiple regression model, but it is more complex.
The mean value of $h_i$ is $\frac{(p+1)}n$. So, if an observation has a leverage statistic that greatly exceeds $\frac{(p+1)}{n}$, then it can be considered a high-leverage point.
\subsubsection*{Collinearity}
\textbf{Collinearity} is a situation in which two or more predictors are highly correlated.

Collinearity can be a problem because it can be difficult to separate out the individual effects of collinear predictors on the response.
When two predictors are collinear, the estimated coefficients of the predictors can be highly variable and can be sensitive to small changes in the model. This derive from the fact that there are a large number of values with similar values of RSS.

The effect is that the standard errors of the estimated coefficients can be large, leading to an increase of the $t$-statistics (because is inversely proportional to $SE(\beta)$) and a decrease of the $p$-values. This means that the model will be less likely to identify the true predictors as statistically significant.
So, the power of hypothesis tests will be reduced and the confidence intervals will be wider than they should be.

A naive way to identify collinearity is to compute the correlation matrix of the predictors. If the correlation between two predictors is close to $1$ or $-1$, then the two predictors are collinear. However, this approach is not always effective because the collinearity can be present on a combination of predictors and not on a single pair. This is called \textbf{multicollinearity}.
A more effective approach is to compute the \textbf{variance inflation factor} (VIF) for each predictor. The VIF is a measure of how much the variance of the estimated coefficient $\hat{\beta}_j$ is inflated due to collinearity. The VIF is defined as:
\[
    VIF(\hat{\beta}_j) = \frac{1}{1-R^2_{X_j|X_{-j}}}
\]
where $R^2_{X_j|X_{-j}}$ is the $R^2$ value obtained by regressing the $j$-th predictor on the remaining predictors. The VIF is always greater than or equal to $1$. If the VIF is equal to $1$, then the $j$-th predictor is completely uncollinear with the other predictors. 
A rule of thumb is that if the VIF is greater than $5$ or $10$, then the $j$-th predictor is highly collinear with the other predictors.

If we find that two predictors are highly collinear, we can remove one of them from the model because one of them is redundant. If we think that both predictors are important, we can combine them into a single predictor but is not so simple to implement this strategy.

\newpage
\section{Supervised Non Parametric Regression}

We are now going to consider the \textbf{non-parametric} or \textbf{distribution-free} setting, meaning that we don't know what are the models that define the data.

The training set is defined as a set of $n$ pairs made by the features $X_i$ and the labels $Y_i$:
\[
    T_n \triangleq    \{(X_i, Y_i)\}_{i=1}^{n}
\]
where $X_i \in \mathbb{R}$ and $Y_i \in \mathbb{R}$. The pairs $(X_i, Y_i)$ are \textit{i.i.d.} samples.

Let $X_0$ be a new observation, we want to predict the label $Y_0$ associated with $X_0$. Since we are focusing on \textit{supervised} non-parametric approaches, the \textit{suboptimal} estimator $\hat{r}(X_0)$ will depend on the training set, and we will call it \textbf{estimated regression function}:
\[
    r_n(X_0) \triangleq r(X_0; T_n)
\]
Since $r_n(X_0)$ is a \textbf{family} of functions, for different realizations of the training set, we get a different function.
% PLOT

\subsection{Consistency}
Suppose we want to compute the error term:
\[
    \E{}{(r_n(X_0) - Y_0)^2} = \text{MMSE} + \E{}{\left(r_n(X_0) - r(X_0)\right)}
\]
But we cannot compute the error term because it depends also on the training set $T_n$, which is another random variable.

In order to solve this problem, we would need to condition the expectation on the training set, by computing the error for a fixed realization of $T_n$:
\[
    \E{}{(r_n(X_0) - Y_0)^2 \mid T_n} = \text{MMSE} + \E{}{\left(r_n(X_0) - r(X_0)\right) \mid T_n} 
\]
Since the MMSE does not depend on the training set, we can ignore the conditioning.

Under the assumption that $(X_0, Y_0)$ is independent on the training set $T_n$, when applying the \textit{tower property}, we obtain:
\[
    \E{}{(r_n(X_0) - Y_0)^2} = \text{MMSE} + \E{}{\left(r_n(X_0) - r(X_0)\right)} 
\]
%TODO: evitare first e second insernedo dei riferimenti specifici
The first expression is a \textbf{random error term}, because the expression depends on the random variable $T_n$, while the second expression is a \textbf{deterministic error term}, because we compute the expectation with respect to all the possible realizations of the training set.

% rivedi questa parte
In practical terms, if I had to implement these two expressions in MATLAB, for the first expression I would need compute it for each single training set, while for the second expression I would need to average over all the possible realizations of the training sets.

\begin{definition}
    The estimator $r_n(X_0)$ is said to be \textbf{consistent} if the
    estimation error $\E{}{(r_n(X_0) - Y_0)^2}$ will converge to the MMSE as $n$ goes to infinity, meaning that the penalty term will go to zero.
\end{definition}
\begin{definition}
    The estimator $r_n(X_0)$ is said to be \textbf{weakly consistent} if:
    \[
        \lim_{n \to +\infty} \E{}{(r_n(X_0) - r(X_0))^2} = 0
    \]
\end{definition}

\begin{definition}
    The estimator $r_n(X_0)$ is said to be \textbf{strongly consistent} if:
    \[
        \lim_{n \to +\infty} \E{}{(r_n(X_0) - r(X_0))^2 \mid T_n} = 0
    \]
\end{definition}
\callout{Note}{We're studying asymptotic behaviors, so $n\to\infty$, because they allows us to appreciate the basic principles on which we can build. For example, LLN is an asymptotic result, because it holds only when $n\to\infty$, but we do extensive use of it altough we know that in practice we will never have infinite samples.}
\subsection{Asymptotic methods}
From the Law of Large Numbers, we know that we can estimate the expected value of a distribution by using the arithmetic mean and we also know that the arithmetic mean is only an approximation of the true mean, which converges to the true mean only with $n \to \infty$.

Even though we know that the arithmetic mean is an approximation, we still use it because it is a universal method, meaning that it does not depend on the specific problem. We can apply the same logic to the regression problem.

An asymptotic method should guarantee that if we collected infinite information, we would get the optimal regression function. Ideally, we want that when $n$ gets large, the penalty term goes to zero, and so we are requesting the \textbf{consistency} of the method.

% plot
%Another remark: when considering a regression problem, we are trying to find $Y_0$ given $X_0$, that can vary in a continuous set. When we take one sample from our dataset:
%\[
%    Y_i = r(X_i) + \mathcal{E}_i
%\]
%this is going to be a single point on the plot.

\paragraph*{Estimation of a conditional probability} We want to find a method that allow us to approximate the optimal regression function:
\[
    r_n(X_0) \approx r(X_0) = \E{}{Y_0 \mid X_0}
\]
In other terms, our goal is to approximate the following conditional probability:
\[
    \E{}{Y_0 \mid X_0 = x_0}
\]
From the definition of conditional probability for discrete variables we know that in order to compute this conditional probability we need to take all of the $Y_i$ values which its $X_i$ is equal to $x_0$ and average the labels of these points. For continuous variables, this definition does not hold because it is highly unlikely to find a value equal to $x_0$.

Let us explain this concept with an example. Suppose we had no information on the data and we only wanted to find the mean of $Y_0$. We could use the arithmetic mean, thanks to the Law of Large Numbers:
\[
    \E{}{Y_0} = \frac{1}{n} \sum Y_i
\]
Given our training samples:
\begin{align*}
     & \{X_i\} = 0,1,0,1,0,1 \dots                  \\
     & \{Y_i\} = y_1, y_2, y_3, y_4, y_5, y_6 \dots
\end{align*}
when we take into account the $X_i$, we are only considering the labels that are associated to the $X_i$ that are equal to $x_0$.

For example, if we wanted to estimate the mean of the labels given $X = 0$, we would compute the arithmetic mean only over the samples were the data is 0:
\[
    \E{}{Y \mid X = 0 } \approx \frac{y_1 + y_3 + y_5 + y_7}{4}
\]
\textcolor{red}{And to find the conditional mean, we would repeat the same thing for every possible $x_i$ in our training set.} %TODO: chiedere a Miriam cosa intende

In general, we can approximate the conditional probability by using the following estimator:
\[
    \E{}{Y \mid X = x} \approx \frac{\sum_{i=1}^n Y_i \cdot \mathbb{I}\{Y_i = r(x)\} }{\sum_{i=1}^n  \mathbb{I}\{Y_i = r(x)\}}
\]
where $\mathbb{I}\{\}$ is the indicator function and it is equal to $1$ if the condition inside brackets is satisfied, and $0$ otherwise.

This approach works correctly when we have a discrete random variable, because we can find the exact value of $X_i$ in the training set. However, if we assume that $X_i$ is a continuous random variable, we know that the event $X_i = x_0$ is not an impossible event but it is highly unlikely because:\footnote{Although the probability is zero, the event is not impossible, which means that we can observe $x_0$.}
\[
    \Pr{X_i = x_0} = 0
\]
Because it is reasonable to assume that $x_0 \notin T_n$. This means that we cannot compute the conditional probability in the same way as we did for the discrete case.

\begin{proof}
    Let's consider the definition of probability by applying a strong law of large numbers:
    \[
        \Pr{X_i = x_0} = \lim_{n \to +\infty} \frac{\#\{X_i = x_0\}}{n} = 0
    \]
    We can see that the probability of the event $X_i = x_0$ is zero in two cases: the first is that the number of occurences of the event is actually zero (and it happens effectively when an event is impossible) and the second case is that the number of the occurrences of the event grows \textbf{sublinearly} or slower than the number of samples.

    Note that the previous equation is valid for any realization of the random variable, which means that we will have \textit{almost surely} the same limit for each realization.

    So we get:

    \[
        \text{Impossible Event } \bar x \implies \#\{X_i=\bar x\} = 0\implies \Pr{X_i = \bar x} = 0     
    \]
    \[
        \Pr{X_i = \bar x} = 0 \not\implies \text{Impossible Event }\bar x
    \]
\end{proof}
\subsection{Naive-Kernel Estimator}
The solution, at least in the approach that we will follow, will consist in a relaxation of the condition $X_0 = x_0$. In particular, we will take the values that lie in a \textbf{neighbourhood} of $x_0$.
\begin{definition}
    The \textbf{naive kernel estimator} is defined as following:
    \[
        r_n^{(NK)}(x_0) = \frac{\sum_{i=1}^n Y_i \quad \mathbb{I}\{\lVert X_i - x_0 \rVert \leq h\} }{\sum_{i=1}^n \mathbb{I}\{\lVert X_i - x_0 \rVert \leq h\}}
    \]
    where $h$ is the size of the neighbourhood $I_h(x_0)$, and $\mathbb{I}(\cdot)$ is the \textbf{indicator function}.
\end{definition}

Note that we used the \textit{euclidean norm} because we are referring to a generic number of dimension $d$. The main problem of this approach is that we don't know how many points will fall into the neighbourhood.

\subsection{Nearest-Neighbour Estimator}
If we wanted a fixed number of points in the neighbourhood, we would need to use another estimator, called the \textbf{nearest neighbour estimator}.

To formally define this estimator, we need to introduce the \textit{sorted list notation}:
\[
    X_{(1)}(x_0), \dots, X_{(n)}(x_0) \text{ s.t. } \lVert X_{(1)}(x_0) - x_0 \rVert \leq \lVert X_{(2)}(x_0) - x_0 \rVert \leq \dots \lVert X_{(n)}(x_0) - x_0 \rVert
\]

Where $X_{(1)}(x_0)$ is the closest sample to $x_0$, $X_{(2)}(x_0)$ is the second closest sample and so on. We can use the following notation to denote also the labels:
\[
    \binom{X_{(1)}(x_0)}{Y_{(1)}(x_0)}, \dots, \binom{X_{(n)}(x_0)}{Y_{(n)}(x_0)}
\]
Note that $Y_{(n)}(x_0)$  is not the $n$-th closest label, but the label associated to the $n$-th closest sample to $x_0$.

\begin{definition}
    The \textbf{nearest neighbour estimator} is defined as following:
    \[
        r_n^{(NN)}(x_0) = \frac{1}{k} \sum_{i=1}^{k} Y_{(i)} (x_0)
    \]
    where $k$ is the number of points in the neighbourhood of $x_0$.
\end{definition}

\subsection{Consistency of the estimators}
\subsubsection{Naive-kernel estimator}
For a good approximation of the conditional probability we need two things: many samples in the neighbourhood of $x_0$ and we need that the neighbourhood is small.

The first condition states that when the number of samples grows, the number of points in the neighbourhood should grow too, this is a consequence of the \textbf{LLN}.
\[
    \frac{\#\{X_i \in I_h(x_0)\}}{n} \stackrel{n \to +\infty}{\approx} \Pr{X_i \in I_h(x_0)} = p > 0
\]
When the number of samples grows, we can approximate the number of points that lie in the neighbourhood over $n$ as the probability that we will find a point in the interval. Since this probability is a positive number $p$, it means that the number of points grows linearly with the number of samples.

The second condition is called \textbf{principle of locality}\footnote{This is the reason why these methods are also called \textbf{local methods}.} and it states that the neighbourhood should be small enough. 

Resuming, in the case of the naive kernel estimator, the mathematical conditions to guarantee both \textbf{LLN} and \textbf{Locality} is that $h \to 0$ when $n \to +\infty$.

There seem to be an apparent contradiction, which is that if we reduce the size of the neighbourhood $h \to 0$, then the number of points in neighbourhood reduces too, thus violating the first condition.

We may naively think that we could consider $h$ as a very small fixed number. But in this case we will converge to an expectation value conditioned to the fact that $X_0\in I_h(x_0)$ where $I_h(x_0)$ is a very small interval, so we lose the locality principle.

The only way to guarantee both conditions is to scale the size of the neighbourhood $h$ along with increasing the number of samples. But this has to happen under certain conditions that we will now introduce.

Firstly, we try to approximate the probability that a point lies in $I_h(x_0)$:
\[
    \Pr{X_i \in I_h(X_0)} \approx f_X(x_0) 2 h_n
\]
because we can assume that if $h$ is very small, then the area under pdf curve of $x_0$ is given by the area of the rectangle with base equal to the size of the neighbourhood $2h$ and height equal to the value of pdf in $x_0$. %TODO: sarebbe bello inserire un'immagine

We also know that we can approximate the probability by the definition given by the frequentist approach:
\[
    \Pr{X_i \in I_n(X_0)} \approx \frac{\#\{X_i \in I_h(x_0)\}}{n} \approx f_X(x_0) 2 h_n
\]
From this, we can derive that:
\[
    \#\{X_i \in I_h(x_0)\} \approx \underbrace{f_X(x_0) 2}_{const} h_n n \iff \#\{X_i \in I_h(x_0)\} \propto h_n n
\]
From this relation, in order to guarantee the the principle of \textit{locality} we want that for $n \to +\infty$, $h_n \to 0$, while to guarantee \textit{LLN} we need that $h_n n \to +\infty$, which are two reasonable requirements which do not conflict. %TODO: check, non sono siocuro che h_n\cdotn\to\infty sia una condizione che si orienti solo alla LLN

\begin{exercise}
    Assume that $h_n$ scales with law $\frac{1}{n^p}$ with $p>1$. Let us verify the \textit{locality} condition:
    \[
        \lim_{n \to +\infty} h_n = \lim_{n \to +\infty} \frac{1}{n^p} = 0
    \]
    Let us verify the \textit{LLN} condition:
    \[
        \lim_{n \to +\infty} h_n n = \lim_{n \to +\infty} \frac{n}{n^p} = 0 \neq +\infty
    \]
    In this case the naive-kernel estimator is not consistent because we are not satisfying the LLN condition.
\end{exercise}

\begin{exercise}
    Assume $h_n$ scales with law $\frac{1}{\sqrt{n}}$. Let us verify the \textit{locality} condition:
    \[
        \lim_{n \to +\infty} h_n = \frac{1}{\sqrt{n}} = 0
    \]
    Let us verify the \textit{LLN} condition:
    \[
        \lim_{n \to +\infty} h_n n = \lim_{n \to +\infty} \frac{n}{\sqrt{n}} = +\infty
    \]
\end{exercise}

We can conclude that for the naive-kernel estimator, $h_n$ should scale \textbf{sublinearly}.

\paragraph*{Multidimensional case}
If we had $d$ dimensions, since we need to compute the volume of the hypercube that approximates the area under the curve, the number of points would be given by:
\[
    \#\{X_i \in I_h(x_0)\} \approx f_x(x_0) 2 h^d_n n
\]
This means that if we increase $d$, $n$ should grow \textbf{exponentially} to match the growth rate of $h$, and this is also known as the \textit{curse of dimensionality}.

Under that condition, can be proved that the nearest neighbour estimator is \textbf{weakly consistent}.
\subsubsection{Nearest-neighbour estimator}
For this estimator, the number of points is given by the parameter $k_n$, so the conditions are reversed with respect to the naive estimator.

To guarantee the \textit{LLN} condition, we need that $k_n \to +\infty$, while to guarantee the \textit{locality} condition, we need that $n \to \infty$ because in this way we're increasing the probability that the $k$ chosen points are closer to $x_0$.

By the way, the two conditions are in conflict and we need a condition that allows us to satisfi both requirements. 

We can simply require that:
\[
    \frac{k_n}{n} \to 0
\]
% Under that condition, can be proved that the nearest neighbour estimator is \textbf{weakly consistent}. %TODO: questa cosa matta l'ha affermata solo per naive kernel

\section{Exercise}
Now we want to implement the theory on non-parametric regression, using the estimators we studied.

Assume we have a random variable $Y$ and a random vector $X \in \mathbb{R}^d$, and we want to estimate the regression function:
\[
    Y = \sin(X) + \mathcal{E}
\]
where $\mathcal{E} \sim N(0,1)$ and $X \sim U(0,a)$.

Let's implement a function that computes the optimal regression function, naive kernel estimator and the nearest neighbour estimator.

Let us now examine the plots.

By selecting $h = 10$ the naive kernel estimator curve has degenerated into a straight line, because the interval is too large with respect to the number of samples, so the estimator just took all of the points and made the average of all the point, that is about zero because the sine is a periodic function. We actually converged to the expectation of $Y$ that is zero.

On the other hand by selecting $k=10$, the nearest neighbour estimator has a shape similar to the one of the sine function.

By selecting $k = 5$ and $h=0.5$, the \textit{naive estimator} starts to take some shape but it is not very similar to the optimal regression function
% i put 10 and 0.1 much better

By changing values, we observed that if $h$ is too small then we will not statisfiy the law of large numbers, and the effect is that...
and if $k$ is too large we lose locality, and the effect is that...

If H is too small i will lose law of large number => 0.01
If K is too larg i will use locality => 50

K = 50 => blue is flattered
H = 0.01 => each point is just the sample, I am enhancing the jumps
-> small H is similar to Knn = 1

if i put K = 100 -> flat line, so i recover large

small k => no LLN
any local avg is not avg but a sample

large K takes all of them but it coverges to the exeception of Y which is close of zero bc we have sin(X)

0.1 and 10

are we statisfied with this learned regression function

we examine what happens with more samples and see if we increase we do better with other number of samples.

1000 of samples
disappointed => blue curve is not nice

Knn =>
if k remains fixed we lose LLN
we are local but the oscillations are great
we are not growing in term of LLN
k must increase with N

recovering LLN

but there are flat => boundary effect because there are not points on the left

increasing K
increasing K

we can acutally converging to the regression function


write this code here and add the analysis of the error
compute the error between these regression function looking at definiction

montecarlo simulation
either with a fixed training set
or
generate a training set each time

for different size of the training set

we need one law that relates k to n or h to n
a
$knn / n \to 0$
$kn \to \infty$
$1/sqrt(N)$

check whether the error is going to zero

check this behaviour of the plot + run simluation
there are not theoretical values


We would expect that the error of the optimal regression function tends to the MMSE, which can be computed as following:
\[
    \text{MMSE} = \E{}{(r(X_0) - Y_0)^2} = \E{}{(\sin(X_0) - \sin(X_0) - \mathcal{E})^2}  = \E{}{\mathcal{E}^2} = 1
\]
